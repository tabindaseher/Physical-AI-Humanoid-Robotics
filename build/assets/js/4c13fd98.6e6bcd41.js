"use strict";(globalThis.webpackChunkbook_docusaurus=globalThis.webpackChunkbook_docusaurus||[]).push([[9121],{5680(e,a,n){n.d(a,{xA:()=>p,yg:()=>d});var t=n(6540);function i(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function s(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter(function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable})),n.push.apply(n,t)}return n}function o(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{};a%2?s(Object(n),!0).forEach(function(a){i(e,a,n[a])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach(function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))})}return e}function r(e,a){if(null==e)return{};var n,t,i=function(e,a){if(null==e)return{};var n,t,i={},s=Object.keys(e);for(t=0;t<s.length;t++)n=s[t],a.indexOf(n)>=0||(i[n]=e[n]);return i}(e,a);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(t=0;t<s.length;t++)n=s[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var c=t.createContext({}),l=function(e){var a=t.useContext(c),n=a;return e&&(n="function"==typeof e?e(a):o(o({},a),e)),n},p=function(e){var a=l(e.components);return t.createElement(c.Provider,{value:a},e.children)},g={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},m=t.forwardRef(function(e,a){var n=e.components,i=e.mdxType,s=e.originalType,c=e.parentName,p=r(e,["components","mdxType","originalType","parentName"]),m=l(n),d=i,u=m["".concat(c,".").concat(d)]||m[d]||g[d]||s;return n?t.createElement(u,o(o({ref:a},p),{},{components:n})):t.createElement(u,o({ref:a},p))});function d(e,a){var n=arguments,i=a&&a.mdxType;if("string"==typeof e||i){var s=n.length,o=new Array(s);o[0]=m;var r={};for(var c in a)hasOwnProperty.call(a,c)&&(r[c]=a[c]);r.originalType=e,r.mdxType="string"==typeof e?e:i,o[1]=r;for(var l=2;l<s;l++)o[l]=n[l];return t.createElement.apply(null,o)}return t.createElement.apply(null,n)}m.displayName="MDXCreateElement"},6744(e,a,n){n.r(a),n.d(a,{assets:()=>c,contentTitle:()=>o,default:()=>g,frontMatter:()=>s,metadata:()=>r,toc:()=>l});var t=n(8168),i=(n(6540),n(5680));const s={sidebar_position:3,title:"Appendix C: Isaac ROS Tutorials"},o="Appendix C: Isaac ROS Tutorials",r={unversionedId:"appendices/appendix-c-isaac-ros-tutorials",id:"appendices/appendix-c-isaac-ros-tutorials",title:"Appendix C: Isaac ROS Tutorials",description:"Overview of Isaac ROS",source:"@site/docs/appendices/appendix-c-isaac-ros-tutorials.md",sourceDirName:"appendices",slug:"/appendices/appendix-c-isaac-ros-tutorials",permalink:"/docs/appendices/appendix-c-isaac-ros-tutorials",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/appendices/appendix-c-isaac-ros-tutorials.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,title:"Appendix C: Isaac ROS Tutorials"},sidebar:"tutorialSidebar",previous:{title:"Appendix B: Gazebo Setup",permalink:"/docs/appendices/appendix-b-gazebo-setup"},next:{title:"Appendix D: VLA Implementation",permalink:"/docs/appendices/appendix-d-vla-implementation"}},c={},l=[{value:"Overview of Isaac ROS",id:"overview-of-isaac-ros",level:2},{value:"Installation",id:"installation",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Setup",id:"setup",level:3},{value:"Getting Started with Isaac ROS Packages",id:"getting-started-with-isaac-ros-packages",level:2},{value:"Isaac ROS Apriltag",id:"isaac-ros-apriltag",level:3},{value:"Isaac ROS Stereo Image Processing",id:"isaac-ros-stereo-image-processing",level:3},{value:"Isaac ROS Visual Simultaneous Localization and Mapping (VSLAM)",id:"isaac-ros-visual-simultaneous-localization-and-mapping-vslam",level:3},{value:"Isaac ROS Navigation",id:"isaac-ros-navigation",level:2},{value:"Basic Navigation Node",id:"basic-navigation-node",level:3},{value:"Isaac ROS Object Detection",id:"isaac-ros-object-detection",level:2},{value:"YOLO-based Object Detection with Isaac ROS",id:"yolo-based-object-detection-with-isaac-ros",level:3},{value:"Isaac ROS with Isaac Sim",id:"isaac-ros-with-isaac-sim",level:2},{value:"Connecting Isaac ROS to Isaac Sim",id:"connecting-isaac-ros-to-isaac-sim",level:3},{value:"Launch Files for Isaac ROS",id:"launch-files-for-isaac-ros",level:2},{value:"Isaac ROS Stereo Example Launch",id:"isaac-ros-stereo-example-launch",level:3},{value:"Isaac ROS AprilTag Example Launch",id:"isaac-ros-apriltag-example-launch",level:3},{value:"TensorRT Integration with Isaac ROS",id:"tensorrt-integration-with-isaac-ros",level:2},{value:"Using TensorRT for Optimized Inference",id:"using-tensorrt-for-optimized-inference",level:3},{value:"Best Practices with Isaac ROS",id:"best-practices-with-isaac-ros",level:2},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Resource Management",id:"resource-management",level:3}],p={toc:l};function g({components:e,...a}){return(0,i.yg)("wrapper",(0,t.A)({},p,a,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"appendix-c-isaac-ros-tutorials"},"Appendix C: Isaac ROS Tutorials"),(0,i.yg)("h2",{id:"overview-of-isaac-ros"},"Overview of Isaac ROS"),(0,i.yg)("p",null,"Isaac ROS is NVIDIA's collection of hardware-accelerated packages that bring the power of NVIDIA's computing platforms to the Robot Operating System (ROS 2). These packages provide GPU-accelerated perception, navigation, and manipulation capabilities for robotics applications."),(0,i.yg)("h2",{id:"installation"},"Installation"),(0,i.yg)("h3",{id:"prerequisites"},"Prerequisites"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"NVIDIA GPU with CUDA support (Compute Capability 6.0+)"),(0,i.yg)("li",{parentName:"ul"},"Ubuntu 20.04 or 22.04"),(0,i.yg)("li",{parentName:"ul"},"ROS 2 Humble Hawksbill"),(0,i.yg)("li",{parentName:"ul"},"NVIDIA Container Toolkit")),(0,i.yg)("h3",{id:"setup"},"Setup"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"# Install Isaac ROS Developer Tools\nsudo apt update\nsudo apt install nvidia-isaa-ros-dev-tools\n\n# Or install specific packages individually\nsudo apt install nvidia-isaac-ros-perceptor\nsudo apt install nvidia-isaac-ros-isaac-ros-nav2\n")),(0,i.yg)("h2",{id:"getting-started-with-isaac-ros-packages"},"Getting Started with Isaac ROS Packages"),(0,i.yg)("h3",{id:"isaac-ros-apriltag"},"Isaac ROS Apriltag"),(0,i.yg)("p",null,"Apriltag detection accelerated on GPU for precise localization."),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-cpp"},'#include <rclcpp/rclcpp.hpp>\n#include <isaac_ros_apriltag_interfaces/msg/april_tag_detection_array.hpp>\n\nclass ApriltagNode : public rclcpp::Node\n{\npublic:\n    ApriltagNode() : Node("apriltag_node")\n    {\n        subscription_ = this->create_subscription<isaac_ros_apriltag_interfaces::msg::AprilTagDetectionArray>(\n            "tag_detections", 10,\n            std::bind(&ApriltagNode::detection_callback, this, std::placeholders::_1));\n    }\n\nprivate:\n    void detection_callback(const isaac_ros_apriltag_interfaces::msg::AprilTagDetectionArray::SharedPtr msg)\n    {\n        RCLCPP_INFO(this->get_logger(), "Detected %zu tags", msg->detections.size());\n        \n        for (const auto& detection : msg->detections) {\n            RCLCPP_INFO(this->get_logger(), \n                "Tag ID: %d, Position: (%.2f, %.2f, %.2f)",\n                detection.id, \n                detection.pose.pose.position.x,\n                detection.pose.pose.position.y,\n                detection.pose.pose.position.z);\n        }\n    }\n    \n    rclcpp::Subscription<isaac_ros_apriltag_interfaces::msg::AprilTagDetectionArray>::SharedPtr subscription_;\n};\n')),(0,i.yg)("h3",{id:"isaac-ros-stereo-image-processing"},"Isaac ROS Stereo Image Processing"),(0,i.yg)("p",null,"Real-time stereo processing accelerated on GPU."),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom stereo_msgs.msg import DisparityImage\n\nclass StereoProcessorNode(Node):\n    def __init__(self):\n        super().__init__('stereo_processor')\n        \n        # Subscribe to stereo image topics\n        self.left_sub = self.create_subscription(\n            Image, '/camera/left/image_rect', self.left_callback, 10)\n        self.right_sub = self.create_subscription(\n            Image, '/camera/right/image_rect', self.right_callback, 10)\n            \n        # Publish disparity map\n        self.disp_pub = self.create_publisher(\n            DisparityImage, '/disparity_map', 10)\n    \n    def left_callback(self, msg):\n        # Process left image\n        self.get_logger().info(f\"Received left image: {msg.width}x{msg.height}\")\n    \n    def right_callback(self, msg):\n        # Process right image\n        self.get_logger().info(f\"Received right image: {msg.width}x{msg.height}\")\n")),(0,i.yg)("h3",{id:"isaac-ros-visual-simultaneous-localization-and-mapping-vslam"},"Isaac ROS Visual Simultaneous Localization and Mapping (VSLAM)"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-cpp"},'#include <rclcpp/rclcpp.hpp>\n#include <nav_msgs/msg/odometry.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n\nclass VSLAMNode : public rclcpp::Node\n{\npublic:\n    VSLAMNode() : Node("vslam_node")\n    {\n        // Subscribe to camera feed\n        image_subscription_ = this->create_subscription<sensor_msgs::msg::Image>(\n            "camera/image", 10,\n            std::bind(&VSLAMNode::image_callback, this, std::placeholders::_1));\n            \n        // Publish odometry\n        odom_publisher_ = this->create_publisher<nav_msgs::msg::Odometry>("odom", 10);\n        \n        // Publish pose\n        pose_publisher_ = this->create_publisher<geometry_msgs::msg::PoseStamped>("pose", 10);\n    }\n\nprivate:\n    void image_callback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        // Process visual SLAM with Isaac ROS VSLAM\n        // This would interface with Isaac ROS VSLAM package\n        RCLCPP_INFO(this->get_logger(), \n            "Processing frame for VSLAM: %dx%d", \n            msg->width, msg->height);\n            \n        // Publish odometry result\n        auto odom_msg = nav_msgs::msg::Odometry();\n        odom_msg.header.stamp = this->get_clock()->now();\n        odom_msg.header.frame_id = "map";\n        odom_publisher_->publish(odom_msg);\n    }\n    \n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_subscription_;\n    rclcpp::Publisher<nav_msgs::msg::Odometry>::SharedPtr odom_publisher_;\n    rclcpp::Publisher<geometry_msgs::msg::PoseStamped>::SharedPtr pose_publisher_;\n};\n')),(0,i.yg)("h2",{id:"isaac-ros-navigation"},"Isaac ROS Navigation"),(0,i.yg)("p",null,"GPU-accelerated navigation stack for mobile robots."),(0,i.yg)("h3",{id:"basic-navigation-node"},"Basic Navigation Node"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom nav2_msgs.action import NavigateToPose\nfrom rclpy.action import ActionClient\n\nclass IsaacNavigationNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_navigation_node\')\n        \n        # Create action client for navigation\n        self.nav_client = ActionClient(\n            self, NavigateToPose, \'navigate_to_pose\')\n        \n        # Create publisher for goal poses\n        self.goal_publisher = self.create_publisher(\n            PoseStamped, \'goal_pose\', 10)\n    \n    def send_goal(self, x, y, z, w=1.0):\n        """Send navigation goal to Isaac ROS Navigation"""\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = \'map\'\n        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.pose.pose.position.x = x\n        goal_msg.pose.pose.position.y = y\n        goal_msg.pose.pose.position.z = 0.0\n        goal_msg.pose.pose.orientation.z = z\n        goal_msg.pose.pose.orientation.w = w\n        \n        # Wait for action server\n        self.nav_client.wait_for_server()\n        \n        # Send goal\n        send_goal_future = self.nav_client.send_goal_async(goal_msg)\n        send_goal_future.add_done_callback(self.goal_response_callback)\n    \n    def goal_response_callback(self, future):\n        """Handle goal response"""\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().info(\'Goal rejected\')\n            return\n            \n        self.get_logger().info(\'Goal accepted\')\n        get_result_future = goal_handle.get_result_async()\n        get_result_future.add_done_callback(self.get_result_callback)\n    \n    def get_result_callback(self, future):\n        """Handle navigation result"""\n        result = future.result().result\n        self.get_logger().info(f\'Navigation completed: {result}\')\n')),(0,i.yg)("h2",{id:"isaac-ros-object-detection"},"Isaac ROS Object Detection"),(0,i.yg)("h3",{id:"yolo-based-object-detection-with-isaac-ros"},"YOLO-based Object Detection with Isaac ROS"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray\nfrom std_msgs.msg import Header\n\nclass IsaacObjectDetectionNode(Node):\n    def __init__(self):\n        super().__init__('isaac_object_detection_node')\n        \n        # Subscribe to camera feed\n        self.image_subscription = self.create_subscription(\n            Image, 'camera/image_raw', self.image_callback, 10)\n        \n        # Publish detections\n        self.detection_publisher = self.create_publisher(\n            Detection2DArray, 'detections', 10)\n    \n    def image_callback(self, msg: Image):\n        \"\"\"\n        Process image and detect objects using Isaac ROS\n        This is a simplified example - real implementation would interface\n        with Isaac ROS object detection packages\n        \"\"\"\n        # Simulate object detection results\n        detection_array = Detection2DArray()\n        detection_array.header = Header()\n        detection_array.header.stamp = self.get_clock().now().to_msg()\n        detection_array.header.frame_id = msg.header.frame_id\n        \n        # Example detection (in reality, this would come from Isaac ROS AI model)\n        if True:  # Condition would be based on actual detection\n            detection = Detection2D()\n            detection.header = detection_array.header\n            \n            # Bounding box (normalized coordinates)\n            bbox = BoundingBox2D()\n            bbox.center.x = 0.5\n            bbox.center.y = 0.5\n            bbox.size_x = 0.2\n            bbox.size_y = 0.3\n            detection.bbox = bbox\n            \n            # Class and confidence\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.id = \"person\"\n            hypothesis.score = 0.95\n            detection.results.append(hypothesis)\n            \n            detection_array.detections.append(detection)\n        \n        # Publish detections\n        self.detection_publisher.publish(detection_array)\n        self.get_logger().info(f'Published {len(detection_array.detections)} detections')\n")),(0,i.yg)("h2",{id:"isaac-ros-with-isaac-sim"},"Isaac ROS with Isaac Sim"),(0,i.yg)("h3",{id:"connecting-isaac-ros-to-isaac-sim"},"Connecting Isaac ROS to Isaac Sim"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2\nfrom geometry_msgs.msg import Twist\nimport numpy as np\n\nclass IsaacSimBridgeNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_sim_bridge\')\n        \n        # Publishers for Isaac Sim\n        self.cmd_vel_publisher = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        \n        # Subscribers from Isaac Sim\n        self.rgb_subscription = self.create_subscription(\n            Image, \'/camera/rgb/image_raw\', self.rgb_callback, 10)\n            \n        self.depth_subscription = self.create_subscription(\n            Image, \'/camera/depth/image_raw\', self.depth_callback, 10)\n            \n        self.camera_info_subscription = self.create_subscription(\n            CameraInfo, \'/camera/rgb/camera_info\', self.camera_info_callback, 10)\n    \n    def rgb_callback(self, msg: Image):\n        """Process RGB camera data from Isaac Sim"""\n        # Convert ROS Image to format usable by Isaac ROS packages\n        # Process with Isaac ROS perception algorithms\n        self.get_logger().info(f\'Received RGB image: {msg.width}x{msg.height}\')\n    \n    def depth_callback(self, msg: Image):\n        """Process depth camera data from Isaac Sim"""\n        # Process depth data with Isaac ROS algorithms\n        self.get_logger().info(f\'Received depth image: {msg.width}x{msg.height}\')\n    \n    def camera_info_callback(self, msg: CameraInfo):\n        """Process camera calibration data"""\n        # Use calibration data for Isaac ROS stereo or monocular processing\n        self.get_logger().info(f\'Camera calibration received\')\n    \n    def send_velocity_command(self, linear_x: float, angular_z: float):\n        """Send velocity command to robot in Isaac Sim"""\n        cmd_msg = Twist()\n        cmd_msg.linear.x = linear_x\n        cmd_msg.angular.z = angular_z\n        self.cmd_vel_publisher.publish(cmd_msg)\n')),(0,i.yg)("h2",{id:"launch-files-for-isaac-ros"},"Launch Files for Isaac ROS"),(0,i.yg)("h3",{id:"isaac-ros-stereo-example-launch"},"Isaac ROS Stereo Example Launch"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"from launch import LaunchDescription\nfrom launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\n\ndef generate_launch_description():\n    # Isaac ROS Stereo Disparity container\n    stereo_disparity_container = ComposableNodeContainer(\n        name='stereo_disparity_container',\n        namespace='',\n        package='rclcpp_components',\n        executable='component_container_mt',\n        composable_node_descriptions=[\n            ComposableNode(\n                package='isaac_ros_stereo_image_proc',\n                plugin='isaac_ros::stereo_image_proc::DisparityNode',\n                name='disparity_node',\n                parameters=[{\n                    'approximate_sync': True,\n                    'use_system_default_qos': True\n                }],\n                remappings=[\n                    ('left/image_rect', '/camera/left/image_rect'),\n                    ('right/image_rect', '/camera/right/image_rect'),\n                    ('left/camera_info', '/camera/left/camera_info'),\n                    ('right/camera_info', '/camera/right/camera_info'),\n                ]\n            )\n        ]\n    )\n\n    return LaunchDescription([stereo_disparity_container])\n")),(0,i.yg)("h3",{id:"isaac-ros-apriltag-example-launch"},"Isaac ROS AprilTag Example Launch"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\n\ndef generate_launch_description():\n    launch_args = [\n        DeclareLaunchArgument(\n            'image_width',\n            default_value='640',\n            description='Width of input images'),\n        DeclareLaunchArgument(\n            'image_height', \n            default_value='480',\n            description='Height of input images'),\n    ]\n\n    image_width = LaunchConfiguration('image_width')\n    image_height = LaunchConfiguration('image_height')\n\n    apriltag_container = ComposableNodeContainer(\n        name='apriltag_container',\n        namespace='apriltag',\n        package='rclcpp_components',\n        executable='component_container_mt',\n        composable_node_descriptions=[\n            ComposableNode(\n                package='isaac_ros_apriltag',\n                plugin='nvidia::isaac_ros::apriltag::AprilTagNode',\n                name='apriltag_node',\n                parameters=[{\n                    'size': 0.32,  # Tag size in meters\n                    'max_tags': 16,\n                    'family': 'tag36h11',\n                }],\n                remappings=[\n                    ('image', '/image_rect'),\n                    ('camera_info', '/camera_info'),\n                ]\n            )\n        ],\n        output='screen'\n    )\n\n    return LaunchDescription(\n        launch_args + [apriltag_container]\n    )\n")),(0,i.yg)("h2",{id:"tensorrt-integration-with-isaac-ros"},"TensorRT Integration with Isaac ROS"),(0,i.yg)("h3",{id:"using-tensorrt-for-optimized-inference"},"Using TensorRT for Optimized Inference"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"import rclpy\nfrom rclpy.node import Node\nimport torch\nimport tensorrt as trt\nimport pycuda.driver as cuda\nimport numpy as np\n\nclass TensorRTInferenceNode(Node):\n    def __init__(self):\n        super().__init__('tensorrt_inference_node')\n        \n        # Initialize TensorRT engine\n        self.engine = self.load_tensorrt_engine('/path/to/model.plan')\n        self.context = self.engine.create_execution_context()\n        \n        # Allocate buffers\n        self.inputs, self.outputs, self.bindings, self.stream = self.allocate_buffers()\n        \n    def load_tensorrt_engine(self, engine_path):\n        \"\"\"Load a pre-built TensorRT engine\"\"\"\n        with open(engine_path, 'rb') as f:\n            engine_data = f.read()\n        \n        runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))\n        engine = runtime.deserialize_cuda_engine(engine_data)\n        return engine\n    \n    def allocate_buffers(self):\n        \"\"\"Allocate buffers for TensorRT engine\"\"\"\n        inputs = []\n        outputs = []\n        bindings = []\n        stream = cuda.Stream()\n        \n        for idx in range(self.engine.num_bindings):\n            print(f\"Binding {idx}: {self.engine.get_binding_name(idx)}\")\n            print(f\"Binding {idx} shape: {self.engine.get_binding_shape(idx)}\")\n            \n            binding_shape = self.engine.get_binding_shape(idx)\n            size = trt.volume(binding_shape) * self.engine.max_batch_size * np.dtype(np.float32).itemsize\n            \n            # Allocate host and device buffers\n            host_mem = cuda.pagelocked_empty(size, dtype=np.float32)\n            device_mem = cuda.mem_alloc(host_mem.nbytes)\n            \n            bindings.append(int(device_mem))\n            \n            if self.engine.binding_is_input(idx):\n                inputs.append({'host': host_mem, 'device': device_mem})\n            else:\n                outputs.append({'host': host_mem, 'device': device_mem})\n        \n        return inputs, outputs, bindings, stream\n    \n    def do_inference(self, input_data):\n        \"\"\"Perform TensorRT inference\"\"\"\n        # Copy input data to GPU\n        np.copyto(self.inputs[0]['host'], input_data.ravel())\n        cuda.memcpy_htod_async(self.inputs[0]['device'], self.inputs[0]['host'], self.stream)\n        \n        # Execute inference\n        self.context.execute_async_v2(bindings=self.bindings, stream_handle=self.stream.handle)\n        \n        # Copy output data back to CPU\n        cuda.memcpy_dtoh_async(self.outputs[0]['host'], self.outputs[0]['device'], self.stream)\n        self.stream.synchronize()\n        \n        return self.outputs[0]['host'].copy()\n")),(0,i.yg)("h2",{id:"best-practices-with-isaac-ros"},"Best Practices with Isaac ROS"),(0,i.yg)("h3",{id:"performance-optimization"},"Performance Optimization"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Use appropriate QoS settings")," for real-time performance"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Minimize data copying")," between host and device memory"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Batch operations")," where possible to maximize GPU utilization"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Profile your application")," to identify bottlenecks")),(0,i.yg)("h3",{id:"safety-considerations"},"Safety Considerations"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Implement safety checks")," before executing actions"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Validate sensor data")," before using it for navigation"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Monitor GPU utilization")," and temperature"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Implement fallback strategies")," when acceleration is not available")),(0,i.yg)("h3",{id:"resource-management"},"Resource Management"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Manage GPU memory")," carefully to avoid out-of-memory errors"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Use appropriate precision")," (FP16 vs FP32) based on application needs"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Monitor power consumption")," for mobile robots"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Implement proper cleanup")," of GPU resources")))}g.isMDXComponent=!0}}]);