"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[9121],{191:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>t,metadata:()=>r,toc:()=>l});var i=a(4848),s=a(8453);const t={sidebar_position:3,title:"Appendix C: Isaac ROS Tutorials"},o="Appendix C: Isaac ROS Tutorials",r={id:"appendices/appendix-c-isaac-ros-tutorials",title:"Appendix C: Isaac ROS Tutorials",description:"Overview of Isaac ROS",source:"@site/docs/appendices/appendix-c-isaac-ros-tutorials.md",sourceDirName:"appendices",slug:"/appendices/appendix-c-isaac-ros-tutorials",permalink:"/docs/appendices/appendix-c-isaac-ros-tutorials",draft:!1,unlisted:!1,editUrl:"https://github.com/tabindaseher/Physical-AI-Humanoid-Robotics/edit/main/docs/appendices/appendix-c-isaac-ros-tutorials.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,title:"Appendix C: Isaac ROS Tutorials"},sidebar:"tutorialSidebar",previous:{title:"Appendix B: Gazebo Setup",permalink:"/docs/appendices/appendix-b-gazebo-setup"},next:{title:"Appendix D: VLA Implementation",permalink:"/docs/appendices/appendix-d-vla-implementation"}},c={},l=[{value:"Overview of Isaac ROS",id:"overview-of-isaac-ros",level:2},{value:"Installation",id:"installation",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Setup",id:"setup",level:3},{value:"Getting Started with Isaac ROS Packages",id:"getting-started-with-isaac-ros-packages",level:2},{value:"Isaac ROS Apriltag",id:"isaac-ros-apriltag",level:3},{value:"Isaac ROS Stereo Image Processing",id:"isaac-ros-stereo-image-processing",level:3},{value:"Isaac ROS Visual Simultaneous Localization and Mapping (VSLAM)",id:"isaac-ros-visual-simultaneous-localization-and-mapping-vslam",level:3},{value:"Isaac ROS Navigation",id:"isaac-ros-navigation",level:2},{value:"Basic Navigation Node",id:"basic-navigation-node",level:3},{value:"Isaac ROS Object Detection",id:"isaac-ros-object-detection",level:2},{value:"YOLO-based Object Detection with Isaac ROS",id:"yolo-based-object-detection-with-isaac-ros",level:3},{value:"Isaac ROS with Isaac Sim",id:"isaac-ros-with-isaac-sim",level:2},{value:"Connecting Isaac ROS to Isaac Sim",id:"connecting-isaac-ros-to-isaac-sim",level:3},{value:"Launch Files for Isaac ROS",id:"launch-files-for-isaac-ros",level:2},{value:"Isaac ROS Stereo Example Launch",id:"isaac-ros-stereo-example-launch",level:3},{value:"Isaac ROS AprilTag Example Launch",id:"isaac-ros-apriltag-example-launch",level:3},{value:"TensorRT Integration with Isaac ROS",id:"tensorrt-integration-with-isaac-ros",level:2},{value:"Using TensorRT for Optimized Inference",id:"using-tensorrt-for-optimized-inference",level:3},{value:"Best Practices with Isaac ROS",id:"best-practices-with-isaac-ros",level:2},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Resource Management",id:"resource-management",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"appendix-c-isaac-ros-tutorials",children:"Appendix C: Isaac ROS Tutorials"}),"\n",(0,i.jsx)(n.h2,{id:"overview-of-isaac-ros",children:"Overview of Isaac ROS"}),"\n",(0,i.jsx)(n.p,{children:"Isaac ROS is NVIDIA's collection of hardware-accelerated packages that bring the power of NVIDIA's computing platforms to the Robot Operating System (ROS 2). These packages provide GPU-accelerated perception, navigation, and manipulation capabilities for robotics applications."}),"\n",(0,i.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,i.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"NVIDIA GPU with CUDA support (Compute Capability 6.0+)"}),"\n",(0,i.jsx)(n.li,{children:"Ubuntu 20.04 or 22.04"}),"\n",(0,i.jsx)(n.li,{children:"ROS 2 Humble Hawksbill"}),"\n",(0,i.jsx)(n.li,{children:"NVIDIA Container Toolkit"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"setup",children:"Setup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Install Isaac ROS Developer Tools\nsudo apt update\nsudo apt install nvidia-isaa-ros-dev-tools\n\n# Or install specific packages individually\nsudo apt install nvidia-isaac-ros-perceptor\nsudo apt install nvidia-isaac-ros-isaac-ros-nav2\n"})}),"\n",(0,i.jsx)(n.h2,{id:"getting-started-with-isaac-ros-packages",children:"Getting Started with Isaac ROS Packages"}),"\n",(0,i.jsx)(n.h3,{id:"isaac-ros-apriltag",children:"Isaac ROS Apriltag"}),"\n",(0,i.jsx)(n.p,{children:"Apriltag detection accelerated on GPU for precise localization."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cpp",children:'#include <rclcpp/rclcpp.hpp>\n#include <isaac_ros_apriltag_interfaces/msg/april_tag_detection_array.hpp>\n\nclass ApriltagNode : public rclcpp::Node\n{\npublic:\n    ApriltagNode() : Node("apriltag_node")\n    {\n        subscription_ = this->create_subscription<isaac_ros_apriltag_interfaces::msg::AprilTagDetectionArray>(\n            "tag_detections", 10,\n            std::bind(&ApriltagNode::detection_callback, this, std::placeholders::_1));\n    }\n\nprivate:\n    void detection_callback(const isaac_ros_apriltag_interfaces::msg::AprilTagDetectionArray::SharedPtr msg)\n    {\n        RCLCPP_INFO(this->get_logger(), "Detected %zu tags", msg->detections.size());\n        \n        for (const auto& detection : msg->detections) {\n            RCLCPP_INFO(this->get_logger(), \n                "Tag ID: %d, Position: (%.2f, %.2f, %.2f)",\n                detection.id, \n                detection.pose.pose.position.x,\n                detection.pose.pose.position.y,\n                detection.pose.pose.position.z);\n        }\n    }\n    \n    rclcpp::Subscription<isaac_ros_apriltag_interfaces::msg::AprilTagDetectionArray>::SharedPtr subscription_;\n};\n'})}),"\n",(0,i.jsx)(n.h3,{id:"isaac-ros-stereo-image-processing",children:"Isaac ROS Stereo Image Processing"}),"\n",(0,i.jsx)(n.p,{children:"Real-time stereo processing accelerated on GPU."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom stereo_msgs.msg import DisparityImage\n\nclass StereoProcessorNode(Node):\n    def __init__(self):\n        super().__init__('stereo_processor')\n        \n        # Subscribe to stereo image topics\n        self.left_sub = self.create_subscription(\n            Image, '/camera/left/image_rect', self.left_callback, 10)\n        self.right_sub = self.create_subscription(\n            Image, '/camera/right/image_rect', self.right_callback, 10)\n            \n        # Publish disparity map\n        self.disp_pub = self.create_publisher(\n            DisparityImage, '/disparity_map', 10)\n    \n    def left_callback(self, msg):\n        # Process left image\n        self.get_logger().info(f\"Received left image: {msg.width}x{msg.height}\")\n    \n    def right_callback(self, msg):\n        # Process right image\n        self.get_logger().info(f\"Received right image: {msg.width}x{msg.height}\")\n"})}),"\n",(0,i.jsx)(n.h3,{id:"isaac-ros-visual-simultaneous-localization-and-mapping-vslam",children:"Isaac ROS Visual Simultaneous Localization and Mapping (VSLAM)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cpp",children:'#include <rclcpp/rclcpp.hpp>\n#include <nav_msgs/msg/odometry.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n\nclass VSLAMNode : public rclcpp::Node\n{\npublic:\n    VSLAMNode() : Node("vslam_node")\n    {\n        // Subscribe to camera feed\n        image_subscription_ = this->create_subscription<sensor_msgs::msg::Image>(\n            "camera/image", 10,\n            std::bind(&VSLAMNode::image_callback, this, std::placeholders::_1));\n            \n        // Publish odometry\n        odom_publisher_ = this->create_publisher<nav_msgs::msg::Odometry>("odom", 10);\n        \n        // Publish pose\n        pose_publisher_ = this->create_publisher<geometry_msgs::msg::PoseStamped>("pose", 10);\n    }\n\nprivate:\n    void image_callback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        // Process visual SLAM with Isaac ROS VSLAM\n        // This would interface with Isaac ROS VSLAM package\n        RCLCPP_INFO(this->get_logger(), \n            "Processing frame for VSLAM: %dx%d", \n            msg->width, msg->height);\n            \n        // Publish odometry result\n        auto odom_msg = nav_msgs::msg::Odometry();\n        odom_msg.header.stamp = this->get_clock()->now();\n        odom_msg.header.frame_id = "map";\n        odom_publisher_->publish(odom_msg);\n    }\n    \n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_subscription_;\n    rclcpp::Publisher<nav_msgs::msg::Odometry>::SharedPtr odom_publisher_;\n    rclcpp::Publisher<geometry_msgs::msg::PoseStamped>::SharedPtr pose_publisher_;\n};\n'})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-ros-navigation",children:"Isaac ROS Navigation"}),"\n",(0,i.jsx)(n.p,{children:"GPU-accelerated navigation stack for mobile robots."}),"\n",(0,i.jsx)(n.h3,{id:"basic-navigation-node",children:"Basic Navigation Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom nav2_msgs.action import NavigateToPose\nfrom rclpy.action import ActionClient\n\nclass IsaacNavigationNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_navigation_node\')\n        \n        # Create action client for navigation\n        self.nav_client = ActionClient(\n            self, NavigateToPose, \'navigate_to_pose\')\n        \n        # Create publisher for goal poses\n        self.goal_publisher = self.create_publisher(\n            PoseStamped, \'goal_pose\', 10)\n    \n    def send_goal(self, x, y, z, w=1.0):\n        """Send navigation goal to Isaac ROS Navigation"""\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = \'map\'\n        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.pose.pose.position.x = x\n        goal_msg.pose.pose.position.y = y\n        goal_msg.pose.pose.position.z = 0.0\n        goal_msg.pose.pose.orientation.z = z\n        goal_msg.pose.pose.orientation.w = w\n        \n        # Wait for action server\n        self.nav_client.wait_for_server()\n        \n        # Send goal\n        send_goal_future = self.nav_client.send_goal_async(goal_msg)\n        send_goal_future.add_done_callback(self.goal_response_callback)\n    \n    def goal_response_callback(self, future):\n        """Handle goal response"""\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().info(\'Goal rejected\')\n            return\n            \n        self.get_logger().info(\'Goal accepted\')\n        get_result_future = goal_handle.get_result_async()\n        get_result_future.add_done_callback(self.get_result_callback)\n    \n    def get_result_callback(self, future):\n        """Handle navigation result"""\n        result = future.result().result\n        self.get_logger().info(f\'Navigation completed: {result}\')\n'})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-ros-object-detection",children:"Isaac ROS Object Detection"}),"\n",(0,i.jsx)(n.h3,{id:"yolo-based-object-detection-with-isaac-ros",children:"YOLO-based Object Detection with Isaac ROS"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray\nfrom std_msgs.msg import Header\n\nclass IsaacObjectDetectionNode(Node):\n    def __init__(self):\n        super().__init__('isaac_object_detection_node')\n        \n        # Subscribe to camera feed\n        self.image_subscription = self.create_subscription(\n            Image, 'camera/image_raw', self.image_callback, 10)\n        \n        # Publish detections\n        self.detection_publisher = self.create_publisher(\n            Detection2DArray, 'detections', 10)\n    \n    def image_callback(self, msg: Image):\n        \"\"\"\n        Process image and detect objects using Isaac ROS\n        This is a simplified example - real implementation would interface\n        with Isaac ROS object detection packages\n        \"\"\"\n        # Simulate object detection results\n        detection_array = Detection2DArray()\n        detection_array.header = Header()\n        detection_array.header.stamp = self.get_clock().now().to_msg()\n        detection_array.header.frame_id = msg.header.frame_id\n        \n        # Example detection (in reality, this would come from Isaac ROS AI model)\n        if True:  # Condition would be based on actual detection\n            detection = Detection2D()\n            detection.header = detection_array.header\n            \n            # Bounding box (normalized coordinates)\n            bbox = BoundingBox2D()\n            bbox.center.x = 0.5\n            bbox.center.y = 0.5\n            bbox.size_x = 0.2\n            bbox.size_y = 0.3\n            detection.bbox = bbox\n            \n            # Class and confidence\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.id = \"person\"\n            hypothesis.score = 0.95\n            detection.results.append(hypothesis)\n            \n            detection_array.detections.append(detection)\n        \n        # Publish detections\n        self.detection_publisher.publish(detection_array)\n        self.get_logger().info(f'Published {len(detection_array.detections)} detections')\n"})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-ros-with-isaac-sim",children:"Isaac ROS with Isaac Sim"}),"\n",(0,i.jsx)(n.h3,{id:"connecting-isaac-ros-to-isaac-sim",children:"Connecting Isaac ROS to Isaac Sim"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2\nfrom geometry_msgs.msg import Twist\nimport numpy as np\n\nclass IsaacSimBridgeNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_sim_bridge\')\n        \n        # Publishers for Isaac Sim\n        self.cmd_vel_publisher = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        \n        # Subscribers from Isaac Sim\n        self.rgb_subscription = self.create_subscription(\n            Image, \'/camera/rgb/image_raw\', self.rgb_callback, 10)\n            \n        self.depth_subscription = self.create_subscription(\n            Image, \'/camera/depth/image_raw\', self.depth_callback, 10)\n            \n        self.camera_info_subscription = self.create_subscription(\n            CameraInfo, \'/camera/rgb/camera_info\', self.camera_info_callback, 10)\n    \n    def rgb_callback(self, msg: Image):\n        """Process RGB camera data from Isaac Sim"""\n        # Convert ROS Image to format usable by Isaac ROS packages\n        # Process with Isaac ROS perception algorithms\n        self.get_logger().info(f\'Received RGB image: {msg.width}x{msg.height}\')\n    \n    def depth_callback(self, msg: Image):\n        """Process depth camera data from Isaac Sim"""\n        # Process depth data with Isaac ROS algorithms\n        self.get_logger().info(f\'Received depth image: {msg.width}x{msg.height}\')\n    \n    def camera_info_callback(self, msg: CameraInfo):\n        """Process camera calibration data"""\n        # Use calibration data for Isaac ROS stereo or monocular processing\n        self.get_logger().info(f\'Camera calibration received\')\n    \n    def send_velocity_command(self, linear_x: float, angular_z: float):\n        """Send velocity command to robot in Isaac Sim"""\n        cmd_msg = Twist()\n        cmd_msg.linear.x = linear_x\n        cmd_msg.angular.z = angular_z\n        self.cmd_vel_publisher.publish(cmd_msg)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"launch-files-for-isaac-ros",children:"Launch Files for Isaac ROS"}),"\n",(0,i.jsx)(n.h3,{id:"isaac-ros-stereo-example-launch",children:"Isaac ROS Stereo Example Launch"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\n\ndef generate_launch_description():\n    # Isaac ROS Stereo Disparity container\n    stereo_disparity_container = ComposableNodeContainer(\n        name='stereo_disparity_container',\n        namespace='',\n        package='rclcpp_components',\n        executable='component_container_mt',\n        composable_node_descriptions=[\n            ComposableNode(\n                package='isaac_ros_stereo_image_proc',\n                plugin='isaac_ros::stereo_image_proc::DisparityNode',\n                name='disparity_node',\n                parameters=[{\n                    'approximate_sync': True,\n                    'use_system_default_qos': True\n                }],\n                remappings=[\n                    ('left/image_rect', '/camera/left/image_rect'),\n                    ('right/image_rect', '/camera/right/image_rect'),\n                    ('left/camera_info', '/camera/left/camera_info'),\n                    ('right/camera_info', '/camera/right/camera_info'),\n                ]\n            )\n        ]\n    )\n\n    return LaunchDescription([stereo_disparity_container])\n"})}),"\n",(0,i.jsx)(n.h3,{id:"isaac-ros-apriltag-example-launch",children:"Isaac ROS AprilTag Example Launch"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\n\ndef generate_launch_description():\n    launch_args = [\n        DeclareLaunchArgument(\n            'image_width',\n            default_value='640',\n            description='Width of input images'),\n        DeclareLaunchArgument(\n            'image_height', \n            default_value='480',\n            description='Height of input images'),\n    ]\n\n    image_width = LaunchConfiguration('image_width')\n    image_height = LaunchConfiguration('image_height')\n\n    apriltag_container = ComposableNodeContainer(\n        name='apriltag_container',\n        namespace='apriltag',\n        package='rclcpp_components',\n        executable='component_container_mt',\n        composable_node_descriptions=[\n            ComposableNode(\n                package='isaac_ros_apriltag',\n                plugin='nvidia::isaac_ros::apriltag::AprilTagNode',\n                name='apriltag_node',\n                parameters=[{\n                    'size': 0.32,  # Tag size in meters\n                    'max_tags': 16,\n                    'family': 'tag36h11',\n                }],\n                remappings=[\n                    ('image', '/image_rect'),\n                    ('camera_info', '/camera_info'),\n                ]\n            )\n        ],\n        output='screen'\n    )\n\n    return LaunchDescription(\n        launch_args + [apriltag_container]\n    )\n"})}),"\n",(0,i.jsx)(n.h2,{id:"tensorrt-integration-with-isaac-ros",children:"TensorRT Integration with Isaac ROS"}),"\n",(0,i.jsx)(n.h3,{id:"using-tensorrt-for-optimized-inference",children:"Using TensorRT for Optimized Inference"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nimport torch\nimport tensorrt as trt\nimport pycuda.driver as cuda\nimport numpy as np\n\nclass TensorRTInferenceNode(Node):\n    def __init__(self):\n        super().__init__('tensorrt_inference_node')\n        \n        # Initialize TensorRT engine\n        self.engine = self.load_tensorrt_engine('/path/to/model.plan')\n        self.context = self.engine.create_execution_context()\n        \n        # Allocate buffers\n        self.inputs, self.outputs, self.bindings, self.stream = self.allocate_buffers()\n        \n    def load_tensorrt_engine(self, engine_path):\n        \"\"\"Load a pre-built TensorRT engine\"\"\"\n        with open(engine_path, 'rb') as f:\n            engine_data = f.read()\n        \n        runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))\n        engine = runtime.deserialize_cuda_engine(engine_data)\n        return engine\n    \n    def allocate_buffers(self):\n        \"\"\"Allocate buffers for TensorRT engine\"\"\"\n        inputs = []\n        outputs = []\n        bindings = []\n        stream = cuda.Stream()\n        \n        for idx in range(self.engine.num_bindings):\n            print(f\"Binding {idx}: {self.engine.get_binding_name(idx)}\")\n            print(f\"Binding {idx} shape: {self.engine.get_binding_shape(idx)}\")\n            \n            binding_shape = self.engine.get_binding_shape(idx)\n            size = trt.volume(binding_shape) * self.engine.max_batch_size * np.dtype(np.float32).itemsize\n            \n            # Allocate host and device buffers\n            host_mem = cuda.pagelocked_empty(size, dtype=np.float32)\n            device_mem = cuda.mem_alloc(host_mem.nbytes)\n            \n            bindings.append(int(device_mem))\n            \n            if self.engine.binding_is_input(idx):\n                inputs.append({'host': host_mem, 'device': device_mem})\n            else:\n                outputs.append({'host': host_mem, 'device': device_mem})\n        \n        return inputs, outputs, bindings, stream\n    \n    def do_inference(self, input_data):\n        \"\"\"Perform TensorRT inference\"\"\"\n        # Copy input data to GPU\n        np.copyto(self.inputs[0]['host'], input_data.ravel())\n        cuda.memcpy_htod_async(self.inputs[0]['device'], self.inputs[0]['host'], self.stream)\n        \n        # Execute inference\n        self.context.execute_async_v2(bindings=self.bindings, stream_handle=self.stream.handle)\n        \n        # Copy output data back to CPU\n        cuda.memcpy_dtoh_async(self.outputs[0]['host'], self.outputs[0]['device'], self.stream)\n        self.stream.synchronize()\n        \n        return self.outputs[0]['host'].copy()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices-with-isaac-ros",children:"Best Practices with Isaac ROS"}),"\n",(0,i.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use appropriate QoS settings"})," for real-time performance"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Minimize data copying"})," between host and device memory"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Batch operations"})," where possible to maximize GPU utilization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Profile your application"})," to identify bottlenecks"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Implement safety checks"})," before executing actions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validate sensor data"})," before using it for navigation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monitor GPU utilization"})," and temperature"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Implement fallback strategies"})," when acceleration is not available"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"resource-management",children:"Resource Management"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Manage GPU memory"})," carefully to avoid out-of-memory errors"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use appropriate precision"})," (FP16 vs FP32) based on application needs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monitor power consumption"})," for mobile robots"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Implement proper cleanup"})," of GPU resources"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>r});var i=a(6540);const s={},t=i.createContext(s);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);