"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[2797],{1894:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>o,metadata:()=>a,toc:()=>c});var t=r(4848),i=r(8453);const o={sidebar_position:1,title:"Chapter 7: API Integration - Conversational Robotics"},s="Chapter 7: API Integration - Conversational Robotics",a={id:"chapter-07/conversational-robotics",title:"Chapter 7: API Integration - Conversational Robotics",description:"Learning Objectives",source:"@site/docs/chapter-07/01-conversational-robotics.md",sourceDirName:"chapter-07",slug:"/chapter-07/conversational-robotics",permalink:"/docs/chapter-07/conversational-robotics",draft:!1,unlisted:!1,editUrl:"https://github.com/tabindaseher/Physical-AI-Humanoid-Robotics/edit/main/docs/chapter-07/01-conversational-robotics.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Chapter 7: API Integration - Conversational Robotics"},sidebar:"tutorialSidebar",previous:{title:"Chapter 6 Exercises",permalink:"/docs/chapter-06/exercises"},next:{title:"Chapter 7 Learning Outcomes",permalink:"/docs/chapter-07/learning-outcomes"}},l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"7.1 Conversational AI Fundamentals",id:"71-conversational-ai-fundamentals",level:2},{value:"Core Components of Conversational Robotics",id:"core-components-of-conversational-robotics",level:3},{value:"Dialogue Management",id:"dialogue-management",level:3},{value:"7.2 Speech Recognition and Synthesis",id:"72-speech-recognition-and-synthesis",level:2},{value:"Automatic Speech Recognition (ASR)",id:"automatic-speech-recognition-asr",level:3},{value:"Dialogue State Tracking",id:"dialogue-state-tracking",level:3},{value:"7.3 Dialog Management",id:"73-dialog-management",level:2},{value:"Intent Recognition and Slot Filling",id:"intent-recognition-and-slot-filling",level:3},{value:"7.4 Embodied Conversational Agents",id:"74-embodied-conversational-agents",level:2},{value:"Multimodal Interaction",id:"multimodal-interaction",level:3},{value:"Social Robotics Principles",id:"social-robotics-principles",level:3},{value:"7.5 Practical Example: Complete Conversational System",id:"75-practical-example-complete-conversational-system",level:2},{value:"7.6 Summary",id:"76-summary",level:2},{value:"7.7 Exercises",id:"77-exercises",level:2},{value:"Exercise 7.1: Speech Recognition System",id:"exercise-71-speech-recognition-system",level:3},{value:"Exercise 7.2: Dialogue State Tracker",id:"exercise-72-dialogue-state-tracker",level:3},{value:"Exercise 7.3: Intent Recognition",id:"exercise-73-intent-recognition",level:3},{value:"Exercise 7.4: Multimodal Integration",id:"exercise-74-multimodal-integration",level:3},{value:"Exercise 7.5: Complete Conversational Agent",id:"exercise-75-complete-conversational-agent",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"chapter-7-api-integration---conversational-robotics",children:"Chapter 7: API Integration - Conversational Robotics"}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you should be able to:"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Remember"}),": List the components of conversational AI systems and their functions in robotics"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Understand"}),": Explain how dialogue systems enable natural human-robot interaction"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Apply"}),": Implement a conversational interface for a robot that processes speech and text"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Analyze"}),": Evaluate the effectiveness of different dialogue strategies in robotics"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Evaluate"}),": Assess the impact of conversational robotics on user experience and engagement"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Create"}),": Design a complete conversational robotics system integrating speech, vision, and action"]}),"\n",(0,t.jsx)(n.h2,{id:"71-conversational-ai-fundamentals",children:"7.1 Conversational AI Fundamentals"}),"\n",(0,t.jsx)(n.p,{children:"Conversational AI in robotics represents the integration of natural language processing, speech recognition, and dialogue management systems to enable human-like interaction with robots. Unlike traditional command-based interfaces, conversational robots can engage in multi-turn dialogues, understand context, and respond appropriately to natural language input."}),"\n",(0,t.jsx)(n.h3,{id:"core-components-of-conversational-robotics",children:"Core Components of Conversational Robotics"}),"\n",(0,t.jsx)(n.p,{children:"The architecture of conversational robots typically includes several key components:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Automatic Speech Recognition (ASR)"}),": Converts spoken language to text"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),": Interprets the meaning of text input"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dialogue Manager"}),": Maintains conversation state and manages turn-taking"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Language Generation (NLG)"}),": Creates natural language responses"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Text-to-Speech (TTS)"}),": Converts text responses to spoken output"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multimodal Integration"}),": Combines speech with visual and other sensory information"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import asyncio\r\nimport numpy as np\r\nimport speech_recognition as sr\r\nfrom typing import Dict, List, Optional, Any\r\nfrom dataclasses import dataclass\r\nfrom enum import Enum\r\n\r\nclass DialogueState(Enum):\r\n    IDLE = "idle"\r\n    LISTENING = "listening"\r\n    PROCESSING = "processing"\r\n    SPEAKING = "speaking"\r\n    WAITING_FOR_CONFIRMATION = "waiting_confirmation"\r\n\r\n@dataclass\r\nclass ConversationContext:\r\n    """Context information for maintaining conversation state"""\r\n    history: List[Dict[str, str]]\r\n    current_intent: str\r\n    entities: Dict[str, Any]\r\n    user_profile: Dict[str, Any]\r\n    session_id: str\r\n    timestamp: float\r\n\r\nclass SpeechRecognitionSystem:\r\n    def __init__(self):\r\n        self.recognizer = sr.Recognizer()\r\n        self.microphone = sr.Microphone()\r\n        \r\n        # Adjust for ambient noise\r\n        with self.microphone as source:\r\n            self.recognizer.adjust_for_ambient_noise(source)\r\n        \r\n        # Set up for specific speech recognition\r\n        self.recognizer.energy_threshold = 400\r\n        self.recognizer.dynamic_energy_threshold = True\r\n        \r\n    def listen_for_speech(self, timeout: float = 5.0) -> Optional[str]:\r\n        """\r\n        Listen for speech input and convert to text\r\n        """\r\n        try:\r\n            with self.microphone as source:\r\n                print("Listening...")\r\n                audio = self.recognizer.listen(source, timeout=timeout)\r\n            \r\n            # Use Google\'s speech recognition service\r\n            text = self.recognizer.recognize_google(audio)\r\n            print(f"Recognized: {text}")\r\n            return text\r\n            \r\n        except sr.WaitTimeoutError:\r\n            print("Timeout: No speech detected")\r\n            return None\r\n        except sr.UnknownValueError:\r\n            print("Could not understand audio")\r\n            return None\r\n        except sr.RequestError as e:\r\n            print(f"Error with speech recognition service: {e}")\r\n            return None\r\n    \r\n    def continuous_listening(self, callback):\r\n        """\r\n        Set up continuous listening with callback for recognized speech\r\n        """\r\n        def audio_callback(recognizer, audio):\r\n            try:\r\n                text = recognizer.recognize_google(audio)\r\n                callback(text)\r\n            except sr.UnknownValueError:\r\n                pass  # Ignore unrecognized audio\r\n            except sr.RequestError:\r\n                pass  # Ignore service errors\r\n        \r\n        self.recognizer.listen_in_background(self.microphone, audio_callback)\r\n\r\nclass NaturalLanguageUnderstanding:\r\n    def __init__(self):\r\n        # Intent classification model (simplified for this example)\r\n        self.intent_keywords = {\r\n            "greeting": ["hello", "hi", "hey", "good morning", "good evening"],\r\n            "navigation": ["go to", "move to", "walk to", "navigate to", "take me to"],\r\n            "object_interaction": ["pick", "grasp", "get", "bring", "fetch", "put", "place"],\r\n            "information_request": ["what", "where", "when", "who", "how", "tell me"],\r\n            "confirmation": ["yes", "no", "ok", "okay", "sure", "cancel"],\r\n            "stop": ["stop", "halt", "pause", "quit", "exit"]\r\n        }\r\n        \r\n        # Entity extraction patterns\r\n        self.location_entities = {\r\n            "kitchen": ["kitchen", "dining area", "cooking area"],\r\n            "living_room": ["living room", "sitting room", "lounge", "family room"],\r\n            "bedroom": ["bedroom", "sleeping area", "bed area"],\r\n            "office": ["office", "study", "workspace", "desk area"],\r\n            "entrance": ["entrance", "front door", "entry", "living area"]\r\n        }\r\n        \r\n    def extract_intent_and_entities(self, text: str) -> Dict[str, Any]:\r\n        """\r\n        Extract intent and entities from input text\r\n        """\r\n        text_lower = text.lower()\r\n        intent = None\r\n        entities = {}\r\n        \r\n        # Extract intent\r\n        for intent_name, keywords in self.intent_keywords.items():\r\n            if any(keyword in text_lower for keyword in keywords):\r\n                intent = intent_name\r\n                break\r\n        \r\n        # Extract location entities\r\n        for location, aliases in self.location_entities.items():\r\n            if any(alias in text_lower for alias in aliases):\r\n                entities[\'location\'] = location\r\n                break\r\n        \r\n        # Extract other entities based on context\r\n        if intent == "object_interaction":\r\n            # Simple object extraction (in practice, this would be more sophisticated)\r\n            words = text_lower.split()\r\n            for i, word in enumerate(words):\r\n                if word in ["the", "a", "an", "some"]:\r\n                    if i + 1 < len(words):\r\n                        entities[\'object\'] = words[i + 1]\r\n                        break\r\n        \r\n        return {\r\n            "intent": intent,\r\n            "entities": entities,\r\n            "confidence": 0.8  # Simplified confidence\r\n        }\n'})}),"\n",(0,t.jsx)(n.h3,{id:"dialogue-management",children:"Dialogue Management"}),"\n",(0,t.jsx)(n.p,{children:"Effective dialogue management is crucial for creating natural, engaging conversations:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class DialogueManager:\r\n    def __init__(self):\r\n        self.current_state = DialogueState.IDLE\r\n        self.conversation_context = ConversationContext(\r\n            history=[],\r\n            current_intent="",\r\n            entities={},\r\n            user_profile={},\r\n            session_id="",\r\n            timestamp=0\r\n        )\r\n        self.context_memory = []  # Recent conversation history\r\n        \r\n    def process_input(self, text: str) -> Dict[str, Any]:\r\n        """\r\n        Process user input and determine appropriate response\r\n        """\r\n        nlu = NaturalLanguageUnderstanding()\r\n        understanding = nlu.extract_intent_and_entities(text)\r\n        \r\n        intent = understanding["intent"]\r\n        entities = understanding["entities"]\r\n        confidence = understanding["confidence"]\r\n        \r\n        # Update conversation context\r\n        self.conversation_context.history.append({\r\n            "user": text,\r\n            "intent": intent,\r\n            "entities": entities,\r\n            "timestamp": self.conversation_context.timestamp\r\n        })\r\n        \r\n        # Update context with new entities\r\n        self.conversation_context.entities.update(entities)\r\n        \r\n        # Determine response based on intent and context\r\n        response = self.generate_response(intent, entities, confidence)\r\n        \r\n        return {\r\n            "response": response,\r\n            "intent": intent,\r\n            "entities": entities,\r\n            "confidence": confidence\r\n        }\r\n    \r\n    def generate_response(self, intent: str, entities: Dict, confidence: float) -> str:\r\n        """\r\n        Generate appropriate response based on intent and context\r\n        """\r\n        if confidence < 0.5:\r\n            return "I\'m sorry, I didn\'t quite understand that. Could you please repeat?"\r\n        \r\n        if intent == "greeting":\r\n            return self.handle_greeting(entities)\r\n        elif intent == "navigation":\r\n            return self.handle_navigation(entities)\r\n        elif intent == "object_interaction":\r\n            return self.handle_object_interaction(entities)\r\n        elif intent == "information_request":\r\n            return self.handle_information_request(entities)\r\n        elif intent == "confirmation":\r\n            return self.handle_confirmation(entities)\r\n        elif intent == "stop":\r\n            return self.handle_stop(entities)\r\n        else:\r\n            # Default response\r\n            if entities:\r\n                return f"I understand you want to do something with the {list(entities.keys())[0]}. Could you please be more specific?"\r\n            else:\r\n                return "I\'m here to help. How can I assist you today?"\r\n    \r\n    def handle_greeting(self, entities: Dict) -> str:\r\n        """Handle greeting intents"""\r\n        import random\r\n        greetings = [\r\n            "Hello! How can I help you today?",\r\n            "Hi there! What can I do for you?",\r\n            "Good to see you! How are you doing?",\r\n            "Hello! I\'m ready to assist you."\r\n        ]\r\n        return random.choice(greetings)\r\n    \r\n    def handle_navigation(self, entities: Dict) -> str:\r\n        """Handle navigation commands"""\r\n        if \'location\' in entities:\r\n            location = entities[\'location\']\r\n            return f"Okay, I\'ll navigate to the {location}. Please follow me."\r\n        else:\r\n            return "I can help you navigate, but I need to know where you\'d like to go."\r\n    \r\n    def handle_object_interaction(self, entities: Dict) -> str:\r\n        """Handle object interaction commands"""\r\n        if \'object\' in entities:\r\n            obj = entities[\'object\']\r\n            return f"I\'ll help you with the {obj}. Can you show me where it is?"\r\n        else:\r\n            return "I can help you interact with objects. What would you like me to help you with?"\r\n    \r\n    def handle_information_request(self, entities: Dict) -> str:\r\n        """Handle information requests"""\r\n        return "I can provide information if you specify what you\'d like to know."\r\n    \r\n    def handle_confirmation(self, entities: Dict) -> str:\r\n        """Handle confirmation responses"""\r\n        return "Got it. What else can I help you with?"\r\n    \r\n    def handle_stop(self, entities: Dict) -> str:\r\n        """Handle stop commands"""\r\n        return "Okay, stopping current operation. How else can I assist you?"\n'})}),"\n",(0,t.jsx)(n.h2,{id:"72-speech-recognition-and-synthesis",children:"7.2 Speech Recognition and Synthesis"}),"\n",(0,t.jsx)(n.h3,{id:"automatic-speech-recognition-asr",children:"Automatic Speech Recognition (ASR)"}),"\n",(0,t.jsx)(n.p,{children:"Advanced speech recognition systems are crucial for conversational robots, allowing them to understand human speech in various conditions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class AdvancedSpeechRecognition:\r\n    def __init__(self):\r\n        self.recognizer = sr.Recognizer()\r\n        self.microphones = sr.Microphone.list_microphone_names()\r\n        \r\n        # Configuration for robotics environment\r\n        self.config = {\r\n            'energy_threshold': 400,\r\n            'dynamic_energy_threshold': True,\r\n            'pause_threshold': 0.8,\r\n            'phrase_threshold': 0.3,\r\n            'non_speaking_duration': 0.5\r\n        }\r\n        \r\n        # Initialize multiple microphones for better hearing\r\n        self.audio_sources = []\r\n        for i, mic_name in enumerate(self.microphones):\r\n            if 'array' in mic_name.lower() or 'beam' in mic_name.lower():\r\n                self.audio_sources.append(sr.Microphone(device_index=i))\r\n        \r\n        # If no special microphones found, use default\r\n        if not self.audio_sources:\r\n            self.audio_sources.append(sr.Microphone())\r\n        \r\n        # Configure recognizer\r\n        self.recognizer.energy_threshold = self.config['energy_threshold']\r\n        self.recognizer.pause_threshold = self.config['pause_threshold']\r\n        self.recognizer.phrase_threshold = self.config['phrase_threshold']\r\n        self.recognizer.non_speaking_duration = self.config['non_speaking_duration']\r\n    \r\n    def adaptive_noise_cancellation(self, source):\r\n        \"\"\"\r\n        Adapt to changing noise conditions\r\n        \"\"\"\r\n        with source:\r\n            self.recognizer.adjust_for_ambient_noise(source, duration=1)\r\n    \r\n    def keyword_spotting(self, keywords: List[str], callback_func, timeout: int = 10):\r\n        \"\"\"\r\n        Listen for specific keywords and call back when detected\r\n        \"\"\"\r\n        import time\r\n        \r\n        keyword_lower = [kw.lower() for kw in keywords]\r\n        \r\n        def listen_loop():\r\n            for source in self.audio_sources:\r\n                with source:\r\n                    try:\r\n                        audio = self.recognizer.listen(source, timeout=timeout)\r\n                        text = self.recognizer.recognize_google(audio).lower()\r\n                        \r\n                        for keyword in keyword_lower:\r\n                            if keyword in text:\r\n                                # Call the callback function with the detected keyword\r\n                                callback_func(keyword, text)\r\n                                return True\r\n                    except sr.WaitTimeoutError:\r\n                        continue\r\n                    except sr.UnknownValueError:\r\n                        continue\r\n                    except sr.RequestError:\r\n                        continue\r\n            return False\r\n        \r\n        # Run the listening loop\r\n        return listen_loop()\r\n    \r\n    def multi_channel_processing(self):\r\n        \"\"\"\r\n        Process audio from multiple channels for better recognition\r\n        \"\"\"\r\n        # In a real implementation, this would handle multiple audio streams\r\n        # for spatial audio processing and noise cancellation\r\n        pass\r\n\r\nclass SpeechSynthesisSystem:\r\n    def __init__(self):\r\n        # For this example, we'll use pyttsx3 for local synthesis\r\n        # In practice, you might use cloud services like AWS Polly or Google TTS\r\n        try:\r\n            import pyttsx3\r\n            self.engine = pyttsx3.init()\r\n            \r\n            # Configure speech properties\r\n            self.engine.setProperty('rate', 150)  # Speed of speech\r\n            self.engine.setProperty('volume', 0.9)  # Volume level (0.0 to 1.0)\r\n            \r\n            # Get available voices and select one that sounds natural\r\n            voices = self.engine.getProperty('voices')\r\n            if voices:\r\n                # Select a natural-sounding voice (usually the first female voice or a specific one)\r\n                for voice in voices:\r\n                    if 'samantha' in voice.name.lower() or 'zira' in voice.name.lower() or 'jane' in voice.name.lower():\r\n                        self.engine.setProperty('voice', voice.id)\r\n                        break\r\n                    elif 'english' in voice.name.lower() or 'en' in voice.name.lower():\r\n                        self.engine.setProperty('voice', voice.id)\r\n                        break\r\n        except ImportError:\r\n            print(\"pyttsx3 not available, using simple print method\")\r\n            self.engine = None\r\n    \r\n    def speak_text(self, text: str, blocking: bool = True):\r\n        \"\"\"\r\n        Convert text to speech and play it\r\n        \"\"\"\r\n        if self.engine:\r\n            self.engine.say(text)\r\n            if blocking:\r\n                self.engine.runAndWait()\r\n            else:\r\n                # Non-blocking speech\r\n                import threading\r\n                def run_speech():\r\n                    self.engine.runAndWait()\r\n                \r\n                thread = threading.Thread(target=run_speech)\r\n                thread.start()\r\n        else:\r\n            # Fallback to simple print\r\n            print(f\"Robot says: {text}\")\r\n    \r\n    def set_speech_properties(self, rate: int = None, volume: float = None, voice: str = None):\r\n        \"\"\"\r\n        Adjust speech synthesis properties\r\n        \"\"\"\r\n        if self.engine:\r\n            if rate is not None:\r\n                self.engine.setProperty('rate', rate)\r\n            if volume is not None:\r\n                self.engine.setProperty('volume', volume)\r\n            if voice is not None:\r\n                self.engine.setProperty('voice', voice)\r\n    \r\n    def preload_voices(self) -> List[str]:\r\n        \"\"\"\r\n        Get list of available voices\r\n        \"\"\"\r\n        if self.engine:\r\n            voices = self.engine.getProperty('voices')\r\n            return [voice.name for voice in voices]\r\n        return []\n"})}),"\n",(0,t.jsx)(n.h3,{id:"dialogue-state-tracking",children:"Dialogue State Tracking"}),"\n",(0,t.jsx)(n.p,{children:"Maintaining context across multiple conversational turns is essential for natural interaction:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from collections import deque\r\nimport json\r\nfrom datetime import datetime\r\n\r\nclass DialogueStateTracker:\r\n    def __init__(self, max_history: int = 10):\r\n        self.max_history = max_history\r\n        self.conversation_history = deque(maxlen=max_history)\r\n        self.current_topic = None\r\n        self.user_context = {\r\n            'preferences': {},\r\n            'conversation_style': 'formal',  # or 'casual'\r\n            'attention_level': 'high'  # 'high', 'medium', 'low'\r\n        }\r\n        self.robot_state = {\r\n            'battery_level': 100.0,\r\n            'current_task': None,\r\n            'location': 'unknown',\r\n            'capabilities': []\r\n        }\r\n        \r\n    def update_context(self, user_input: str, robot_response: str, intent: str, entities: Dict):\r\n        \"\"\"\r\n        Update the dialogue context with new information\r\n        \"\"\"\r\n        timestamp = datetime.now().isoformat()\r\n        \r\n        # Create conversation entry\r\n        entry = {\r\n            'timestamp': timestamp,\r\n            'user_input': user_input,\r\n            'robot_response': robot_response,\r\n            'intent': intent,\r\n            'entities': entities,\r\n            'context': self.user_context.copy(),\r\n            'robot_state': self.robot_state.copy()\r\n        }\r\n        \r\n        self.conversation_history.append(entry)\r\n        \r\n        # Update current topic based on intent\r\n        if intent in ['navigation', 'object_interaction', 'information_request']:\r\n            self.current_topic = intent\r\n        \r\n        # Update user context based on interaction\r\n        self._update_user_preferences(user_input, intent)\r\n        self._update_attention_level()\r\n    \r\n    def _update_user_preferences(self, user_input: str, intent: str):\r\n        \"\"\"\r\n        Update user preferences based on interaction patterns\r\n        \"\"\"\r\n        # Simple learning of user preferences\r\n        if intent == \"navigation\":\r\n            # Remember preferred locations\r\n            pass\r\n        elif intent == \"object_interaction\":\r\n            # Remember object preferences\r\n            pass\r\n    \r\n    def _update_attention_level(self):\r\n        \"\"\"\r\n        Update attention level based on conversation dynamics\r\n        \"\"\"\r\n        # This would use timing, engagement indicators, etc.\r\n        # For now, we'll keep it simple\r\n        pass\r\n    \r\n    def get_relevant_context(self, current_intent: str) -> Dict:\r\n        \"\"\"\r\n        Get context relevant to current intent\r\n        \"\"\"\r\n        relevant_context = {}\r\n        \r\n        # Look for recent entries with similar intent\r\n        for entry in list(self.conversation_history)[-3:]:  # Check last 3 entries\r\n            if entry['intent'] == current_intent:\r\n                # Include entities from similar intents\r\n                relevant_context.update(entry['entities'])\r\n        \r\n        # Add current user and robot state\r\n        relevant_context['user_context'] = self.user_context\r\n        relevant_context['robot_state'] = self.robot_state\r\n        \r\n        return relevant_context\r\n    \r\n    def serialize_context(self) -> str:\r\n        \"\"\"\r\n        Serialize current context for storage or transmission\r\n        \"\"\"\r\n        context_data = {\r\n            'history': list(self.conversation_history),\r\n            'current_topic': self.current_topic,\r\n            'user_context': self.user_context,\r\n            'robot_state': self.robot_state\r\n        }\r\n        return json.dumps(context_data, default=str)\r\n    \r\n    def deserialize_context(self, context_str: str):\r\n        \"\"\"\r\n        Deserialize context from storage\r\n        \"\"\"\r\n        context_data = json.loads(context_str)\r\n        self.conversation_history = deque(context_data['history'], maxlen=self.max_history)\r\n        self.current_topic = context_data['current_topic']\r\n        self.user_context = context_data['user_context']\r\n        self.robot_state = context_data['robot_state']\n"})}),"\n",(0,t.jsx)(n.h2,{id:"73-dialog-management",children:"7.3 Dialog Management"}),"\n",(0,t.jsx)(n.h3,{id:"intent-recognition-and-slot-filling",children:"Intent Recognition and Slot Filling"}),"\n",(0,t.jsx)(n.p,{children:"Effective dialogue systems need to understand user intent and extract relevant information:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import re\r\nfrom typing import Tuple\r\n\r\nclass IntentRecognitionSystem:\r\n    def __init__(self):\r\n        # Define intent patterns with regex and keywords\r\n        self.intent_patterns = {\r\n            'navigation_to_location': {\r\n                'patterns': [\r\n                    r'go to (?:the )?(\\w+)',\r\n                    r'move to (?:the )?(\\w+)',\r\n                    r'walk to (?:the )?(\\w+)',\r\n                    r'navigate to (?:the )?(\\w+)',\r\n                    r'can you take me to (?:the )?(\\w+)'\r\n                ],\r\n                'entities': ['location']\r\n            },\r\n            'grasp_object': {\r\n                'patterns': [\r\n                    r'pick up (?:the )?(\\w+)',\r\n                    r'grasp (?:the )?(\\w+)',\r\n                    r'get (?:the )?(\\w+)',\r\n                    r'bring me (?:the )?(\\w+)',\r\n                    r'fetch (?:the )?(\\w+)'\r\n                ],\r\n                'entities': ['object']\r\n            },\r\n            'place_object': {\r\n                'patterns': [\r\n                    r'put (?:the )?(\\w+) in (?:the )?(\\w+)',\r\n                    r'place (?:the )?(\\w+) on (?:the )?(\\w+)',\r\n                    r'put (?:the )?(\\w+) (?:in|on|at) (?:the )?(\\w+)'\r\n                ],\r\n                'entities': ['object', 'location']\r\n            },\r\n            'greeting': {\r\n                'patterns': [\r\n                    r'hello',\r\n                    r'hi',\r\n                    r'hey',\r\n                    r'good morning',\r\n                    r'good evening',\r\n                    r'good afternoon'\r\n                ],\r\n                'entities': []\r\n            },\r\n            'time_request': {\r\n                'patterns': [\r\n                    r'what time is it',\r\n                    r'what is the time',\r\n                    r'tell me the time',\r\n                    r'current time',\r\n                    r'clock'\r\n                ],\r\n                'entities': []\r\n            }\r\n        }\r\n        \r\n    def recognize_intent(self, text: str) -> Tuple[str, Dict[str, str]]:\r\n        \"\"\"\r\n        Recognize intent and extract entities from text\r\n        \"\"\"\r\n        text_lower = text.lower().strip()\r\n        \r\n        for intent_name, intent_data in self.intent_patterns.items():\r\n            for pattern in intent_data['patterns']:\r\n                match = re.search(pattern, text_lower)\r\n                if match:\r\n                    entities = {}\r\n                    groups = match.groups()\r\n                    \r\n                    # Extract entities based on pattern groups\r\n                    for i, entity_type in enumerate(intent_data['entities']):\r\n                        if i < len(groups):\r\n                            entities[entity_type] = groups[i]\r\n                    \r\n                    return intent_name, entities\r\n        \r\n        # If no pattern matches, use keyword-based recognition\r\n        return self._keyword_based_recognition(text_lower)\r\n    \r\n    def _keyword_based_recognition(self, text_lower: str) -> Tuple[str, Dict[str, str]]:\r\n        \"\"\"\r\n        Fallback to keyword-based intent recognition\r\n        \"\"\"\r\n        # Simple keyword matching as fallback\r\n        if any(word in text_lower for word in ['hello', 'hi', 'hey']):\r\n            return 'greeting', {}\r\n        elif any(word in text_lower for word in ['time', 'clock', 'hour']):\r\n            return 'time_request', {}\r\n        elif any(word in text_lower for word in ['go', 'move', 'navigate', 'walk']):\r\n            return 'navigation_to_location', {}\r\n        elif any(word in text_lower for word in ['pick', 'grasp', 'get', 'fetch']):\r\n            return 'grasp_object', {}\r\n        else:\r\n            return 'unknown', {}\r\n\r\nclass SlotFillingSystem:\r\n    def __init__(self):\r\n        self.required_slots = {\r\n            'navigation_to_location': ['location'],\r\n            'grasp_object': ['object'],\r\n            'place_object': ['object', 'destination'],\r\n            'order_delivery': ['item', 'destination']\r\n        }\r\n        \r\n        self.filled_slots = {}\r\n        self.current_intent = None\r\n    \r\n    def start_slot_filling(self, intent: str):\r\n        \"\"\"\r\n        Start slot filling for a particular intent\r\n        \"\"\"\r\n        self.current_intent = intent\r\n        self.filled_slots = {}\r\n        \r\n        if intent in self.required_slots:\r\n            required = self.required_slots[intent]\r\n            for slot in required:\r\n                self.filled_slots[slot] = None\r\n    \r\n    def fill_slots(self, entities: Dict[str, str]) -> Dict[str, str]:\r\n        \"\"\"\r\n        Fill slots with provided entities\r\n        \"\"\"\r\n        unfilled_slots = []\r\n        \r\n        for slot, value in entities.items():\r\n            if slot in self.filled_slots:\r\n                self.filled_slots[slot] = value\r\n        \r\n        # Check which slots are still unfilled\r\n        for slot, value in self.filled_slots.items():\r\n            if value is None:\r\n                unfilled_slots.append(slot)\r\n        \r\n        return {\r\n            'filled_slots': self.filled_slots.copy(),\r\n            'unfilled_slots': unfilled_slots,\r\n            'completed': len(unfilled_slots) == 0\r\n        }\r\n    \r\n    def is_slot_filling_complete(self) -> bool:\r\n        \"\"\"\r\n        Check if all required slots are filled\r\n        \"\"\"\r\n        if not self.current_intent:\r\n            return False\r\n            \r\n        required_slots = self.required_slots.get(self.current_intent, [])\r\n        for slot in required_slots:\r\n            if self.filled_slots.get(slot) is None:\r\n                return False\r\n        return True\r\n    \r\n    def get_missing_slots(self) -> List[str]:\r\n        \"\"\"\r\n        Get a list of missing slots\r\n        \"\"\"\r\n        if not self.current_intent:\r\n            return []\r\n        \r\n        missing = []\r\n        required_slots = self.required_slots.get(self.current_intent, [])\r\n        for slot in required_slots:\r\n            if self.filled_slots.get(slot) is None:\r\n                missing.append(slot)\r\n        return missing\n"})}),"\n",(0,t.jsx)(n.h2,{id:"74-embodied-conversational-agents",children:"7.4 Embodied Conversational Agents"}),"\n",(0,t.jsx)(n.h3,{id:"multimodal-interaction",children:"Multimodal Interaction"}),"\n",(0,t.jsx)(n.p,{children:"Conversational robots must integrate speech, vision, and action for natural interaction:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import cv2\r\nimport numpy as np\r\nfrom threading import Thread\r\nfrom queue import Queue\r\nimport time\r\n\r\nclass MultimodalInteractionManager:\r\n    def __init__(self):\r\n        self.vision_system = VisionSystem()\r\n        self.speech_system = SpeechRecognitionSystem()\r\n        self.dialogue_manager = DialogueManager()\r\n        self.action_executor = ActionExecutor()\r\n        \r\n        # Queues for asynchronous processing\r\n        self.vision_queue = Queue(maxsize=5)\r\n        self.speech_queue = Queue(maxsize=5)\r\n        self.response_queue = Queue(maxsize=5)\r\n        \r\n        # State tracking\r\n        self.attention_target = None\r\n        self.gaze_direction = (0, 0)  # In terms of head angles\r\n        self.social_signals = {}\r\n        \r\n        # Processing threads\r\n        self.vision_thread = None\r\n        self.speech_thread = None\r\n        self.response_thread = None\r\n        \r\n    def start_listening(self):\r\n        """\r\n        Start listening for speech input\r\n        """\r\n        def speech_processing_loop():\r\n            while True:\r\n                try:\r\n                    text = self.speech_system.listen_for_speech()\r\n                    if text:\r\n                        self.speech_queue.put_nowait(text)\r\n                except Exception as e:\r\n                    print(f"Speech processing error: {e}")\r\n                time.sleep(0.1)\r\n        \r\n        self.speech_thread = Thread(target=speech_processing_loop, daemon=True)\r\n        self.speech_thread.start()\r\n    \r\n    def start_vision_processing(self):\r\n        """\r\n        Start processing visual input\r\n        """\r\n        def vision_processing_loop():\r\n            while True:\r\n                try:\r\n                    # Get visual data (in a real system, this would capture from cameras)\r\n                    visual_data = self.vision_system.get_current_scene()\r\n                    if visual_data:\r\n                        self.vision_queue.put_nowait(visual_data)\r\n                except Exception as e:\r\n                    print(f"Vision processing error: {e}")\r\n                time.sleep(0.033)  # ~30 FPS\r\n        \r\n        self.vision_thread = Thread(target=vision_processing_loop, daemon=True)\r\n        self.vision_thread.start()\r\n    \r\n    def start_response_processing(self):\r\n        """\r\n        Start processing responses\r\n        """\r\n        def response_processing_loop():\r\n            while True:\r\n                try:\r\n                    # Process speech input\r\n                    speech_input = None\r\n                    try:\r\n                        speech_input = self.speech_queue.get_nowait()\r\n                    except:\r\n                        pass\r\n                    \r\n                    if speech_input:\r\n                        # Process the speech input with context from vision\r\n                        visual_context = None\r\n                        try:\r\n                            visual_context = self.vision_queue.get_nowait()\r\n                        except:\r\n                            pass\r\n                        \r\n                        response = self.process_multimodal_input(speech_input, visual_context)\r\n                        self.response_queue.put_nowait(response)\r\n                        \r\n                        # Execute speech response\r\n                        if response:\r\n                            self.action_executor.speak(response)\r\n                \r\n                except Exception as e:\r\n                    print(f"Response processing error: {e}")\r\n                time.sleep(0.1)\r\n        \r\n        self.response_thread = Thread(target=response_processing_loop, daemon=True)\r\n        self.response_thread.start()\r\n    \r\n    def process_multimodal_input(self, speech_input: str, visual_context: Dict = None):\r\n        """\r\n        Process input that combines speech and visual information\r\n        """\r\n        # Extract intent and entities from speech\r\n        dialogue_result = self.dialogue_manager.process_input(speech_input)\r\n        \r\n        intent = dialogue_result.get(\'intent\', \'unknown\')\r\n        entities = dialogue_result.get(\'entities\', {})\r\n        \r\n        # Integrate visual context\r\n        if visual_context and entities.get(\'object\'):\r\n            # Match the mentioned object with what\'s seen in the visual data\r\n            matched_object = self.vision_system.find_object_by_name(\r\n                entities[\'object\'], \r\n                visual_context\r\n            )\r\n            if matched_object:\r\n                entities[\'object_location\'] = matched_object[\'location\']\r\n        \r\n        # Generate response based on multimodal input\r\n        response = self.generate_contextual_response(\r\n            intent, \r\n            entities, \r\n            visual_context\r\n        )\r\n        \r\n        # Update attention and gaze based on interaction\r\n        self.update_attention(speech_input, visual_context)\r\n        \r\n        return response\r\n    \r\n    def generate_contextual_response(self, intent: str, entities: Dict, visual_context: Dict = None):\r\n        """\r\n        Generate response considering both speech and visual context\r\n        """\r\n        if intent == "greeting":\r\n            return self._generate_greeting_response(visual_context)\r\n        elif intent == "object_interaction":\r\n            if entities.get(\'object\'):\r\n                return self._generate_object_interaction_response(entities, visual_context)\r\n        elif intent == "navigation":\r\n            return self._generate_navigation_response(entities, visual_context)\r\n        else:\r\n            return "I understand you\'re trying to communicate with me. How can I help you?"\r\n    \r\n    def _generate_greeting_response(self, visual_context: Dict = None):\r\n        """\r\n        Generate greeting response with visual context\r\n        """\r\n        if visual_context:\r\n            # Check if person is recognized\r\n            person_detected = visual_context.get(\'person_detected\', False)\r\n            if person_detected:\r\n                return "Hello! It\'s good to see you again."\r\n            else:\r\n                return "Hello! I\'m here to help you. What can I do for you?"\r\n        else:\r\n            return "Hello! I\'m here to help you. What can I do for you?"\r\n    \r\n    def _generate_object_interaction_response(self, entities: Dict, visual_context: Dict = None):\r\n        """\r\n        Generate response for object interaction with visual context\r\n        """\r\n        obj_name = entities.get(\'object\', \'object\')\r\n        \r\n        if visual_context:\r\n            # Check if object is visible\r\n            obj_found = self.vision_system.find_object_by_name(obj_name, visual_context)\r\n            if obj_found:\r\n                return f"I can see the {obj_name}. Where would you like me to {entities.get(\'action\', \'get\')} it?"\r\n            else:\r\n                return f"I don\'t see a {obj_name} nearby. Could you point it out or tell me where it is?"\r\n        else:\r\n            return f"I can help you with the {obj_name}. Can you show me where it is?"\r\n    \r\n    def _generate_navigation_response(self, entities: Dict, visual_context: Dict = None):\r\n        """\r\n        Generate navigation response with visual context\r\n        """\r\n        location = entities.get(\'location\', \'destination\')\r\n        \r\n        # Check if location is accessible\r\n        if self.action_executor.is_location_reachable(location):\r\n            return f"Okay, I\'ll navigate to the {location}. Please follow me."\r\n        else:\r\n            return f"I can help you get to the {location}, but I need more specific directions."\r\n    \r\n    def update_attention(self, speech_input: str, visual_context: Dict = None):\r\n        """\r\n        Update robot\'s attention based on multimodal input\r\n        """\r\n        # Update gaze based on who is speaking\r\n        if visual_context and visual_context.get(\'person_detected\'):\r\n            # Calculate gaze direction toward the person\r\n            person_location = visual_context.get(\'person_location\', (0, 0))\r\n            self.gaze_direction = self.calculate_gaze_direction(person_location)\r\n        \r\n        # Track attention target\r\n        if "you" in speech_input.lower():\r\n            self.attention_target = "user"\r\n        elif visual_context:\r\n            # Update attention based on visual focus\r\n            self.attention_target = "environment"\r\n    \r\n    def calculate_gaze_direction(self, target_location: Tuple[float, float]):\r\n        """\r\n        Calculate head angles to look at a target location\r\n        """\r\n        # Simplified calculation - in reality, this would involve\r\n        # inverse kinematics for the neck/head joints\r\n        x, y = target_location\r\n        # Convert to head angles (simplified)\r\n        head_yaw = np.arctan2(y, x)  # Yaw angle\r\n        head_pitch = 0.0  # Keep pitch level for now\r\n        \r\n        return (head_yaw, head_pitch)\r\n\r\nclass VisionSystem:\r\n    def __init__(self):\r\n        # Initialize camera and computer vision components\r\n        self.camera = None\r\n        self.object_detector = None  # Would be a trained model in practice\r\n        self.face_recognizer = None  # Would be trained for user recognition\r\n        \r\n    def get_current_scene(self) -> Dict:\r\n        """\r\n        Get current visual information from the environment\r\n        """\r\n        # In a real implementation, this would capture from camera\r\n        # and process with computer vision algorithms\r\n        \r\n        # For simulation, return mock data\r\n        return {\r\n            \'objects\': [\r\n                {\'name\': \'mug\', \'location\': (1.2, 0.5, 0.8), \'confidence\': 0.9},\r\n                {\'name\': \'book\', \'location\': (1.5, 0.2, 0.8), \'confidence\': 0.85}\r\n            ],\r\n            \'person_detected\': True,\r\n            \'person_location\': (0.8, 0, 1.0),  # x, y, z coordinates relative to robot\r\n            \'timestamp\': time.time()\r\n        }\r\n    \r\n    def find_object_by_name(self, obj_name: str, scene_data: Dict) -> Dict:\r\n        """\r\n        Find an object by name in the current scene\r\n        """\r\n        for obj in scene_data.get(\'objects\', []):\r\n            if obj_name.lower() in obj[\'name\'].lower() or obj_name.lower() == obj[\'name\'].lower():\r\n                return obj\r\n        return None\r\n    \r\n    def get_people_info(self, scene_data: Dict) -> List[Dict]:\r\n        """\r\n        Get information about people in the scene\r\n        """\r\n        # Would implement face recognition and tracking\r\n        return scene_data.get(\'people\', [])\r\n\r\nclass ActionExecutor:\r\n    def __init__(self):\r\n        self.speech_synthesis = SpeechSynthesisSystem()\r\n        self.robot_controls = None  # Would connect to actual robot\r\n        self.animation_player = None  # Would handle robot animations\r\n        \r\n    def speak(self, text: str):\r\n        """\r\n        Make the robot speak the given text\r\n        """\r\n        self.speech_synthesis.speak_text(text)\r\n    \r\n    def move_to_location(self, location: str):\r\n        """\r\n        Navigate the robot to a specified location\r\n        """\r\n        # In a real implementation, this would:\r\n        # - Plan a path to the location\r\n        # - Execute navigation commands\r\n        # - Monitor progress\r\n        print(f"Moving to {location}")\r\n    \r\n    def grasp_object(self, obj_name: str):\r\n        """\r\n        Execute grasping action for a named object\r\n        """\r\n        # In a real implementation, this would:\r\n        # - Locate the object using vision\r\n        # - Plan grasp trajectory\r\n        # - Execute grasp with manipulator\r\n        print(f"Grasping {obj_name}")\r\n    \r\n    def is_location_reachable(self, location: str) -> bool:\r\n        """\r\n        Check if a location is reachable by the robot\r\n        """\r\n        # Check against map or navigation system\r\n        reachable_locations = [\'kitchen\', \'living_room\', \'office\', \'bedroom\', \'entrance\']\r\n        return location.lower() in [loc.lower() for loc in reachable_locations]\n'})}),"\n",(0,t.jsx)(n.h3,{id:"social-robotics-principles",children:"Social Robotics Principles"}),"\n",(0,t.jsx)(n.p,{children:"Creating robots that can interact naturally with humans involves understanding social cues and responses:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class SocialRoboticsEngine:\r\n    def __init__(self):\r\n        self.social_rules = {\r\n            \'personal_space\': 1.0,  # meters\r\n            \'greeting_protocols\': {\r\n                \'time_based\': True,\r\n                \'frequency\': \'first_meeting_only\',\r\n                \'duration\': 3  # seconds\r\n            },\r\n            \'attention_management\': {\r\n                \'focus_time\': 5,  # seconds before shifting attention\r\n                \'polite_attention_shifting\': True\r\n            }\r\n        }\r\n        \r\n        self.user_models = {}  # Store models of different users\r\n        self.social_context = {\r\n            \'cultural_background\': \'universal\',\r\n            \'age_group\': \'adult\',\r\n            \'formality_level\': \'medium\'\r\n        }\r\n        \r\n    def handle_greeting(self, user_id: str, context: Dict = None):\r\n        """\r\n        Handle greeting with appropriate social awareness\r\n        """\r\n        # Check if this is a returning user\r\n        is_known_user = user_id in self.user_models\r\n        time_of_day = context.get(\'time_of_day\', \'day\')\r\n        \r\n        greeting = self._select_greeting(is_known_user, time_of_day)\r\n        \r\n        # Add personalization if available\r\n        if is_known_user and \'name\' in self.user_models[user_id]:\r\n            name = self.user_models[user_id][\'name\']\r\n            greeting = f"{greeting} {name}!"\r\n        else:\r\n            greeting = f"{greeting}! I\'m your assistant robot."\r\n        \r\n        return greeting\r\n    \r\n    def _select_greeting(self, is_known_user: bool, time_of_day: str) -> str:\r\n        """\r\n        Select appropriate greeting based on context\r\n        """\r\n        import random\r\n        \r\n        if not is_known_user:\r\n            greetings = [\r\n                "Hello",\r\n                "Hi there",\r\n                "Greetings",\r\n                "Nice to meet you"\r\n            ]\r\n        else:\r\n            if time_of_day == \'morning\':\r\n                greetings = ["Good morning", "Good to see you again this morning"]\r\n            elif time_of_day == \'evening\':\r\n                greetings = ["Good evening", "Hello again"]\r\n            else:\r\n                greetings = ["Hello", "Hi", "Good to see you"]\r\n        \r\n        return random.choice(greetings)\r\n    \r\n    def manage_attention(self, current_user: str, detected_users: List[str]):\r\n        """\r\n        Manage attention between multiple users\r\n        """\r\n        if len(detected_users) == 1:\r\n            # Single user interaction\r\n            return current_user\r\n        elif len(detected_users) == 0:\r\n            # No users detected\r\n            return None\r\n        else:\r\n            # Multiple users - implement social rules\r\n            return self._handle_multiple_users(current_user, detected_users)\r\n    \r\n    def _handle_multiple_users(self, current_user: str, detected_users: List[str]) -> str:\r\n        """\r\n        Handle attention when multiple users are present\r\n        """\r\n        # Follow social rules for attention management\r\n        # This might involve: politeness, turn-taking, etc.\r\n        if current_user in detected_users:\r\n            # Continue with current user\r\n            return current_user\r\n        else:\r\n            # Switch to a new user\r\n            # For now, pick the first one\r\n            return detected_users[0]\r\n    \r\n    def generate_social_responses(self, context: Dict) -> Dict:\r\n        """\r\n        Generate socially appropriate responses\r\n        """\r\n        response_type = context.get(\'response_type\', \'standard\')\r\n        \r\n        responses = {\r\n            \'appreciation\': [\r\n                "Thank you for your patience.",\r\n                "I appreciate your understanding.",\r\n                "Thank you for working with me."\r\n            ],\r\n            \'clarification\': [\r\n                "Could you please repeat that?",\r\n                "I didn\'t quite catch that, could you say it again?",\r\n                "I need a bit more information."\r\n            ],\r\n            \'error_recovery\': [\r\n                "I apologize for the confusion.",\r\n                "Let me try that again.",\r\n                "I seem to have misunderstood. Can you help me?"\r\n            ]\r\n        }\r\n        \r\n        import random\r\n        return responses.get(response_type, responses[\'appreciation\'])\n'})}),"\n",(0,t.jsx)(n.h2,{id:"75-practical-example-complete-conversational-system",children:"7.5 Practical Example: Complete Conversational System"}),"\n",(0,t.jsx)(n.p,{children:"Let's integrate all the components into a complete conversational robotics system:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom geometry_msgs.msg import Pose, Twist\r\nfrom rclpy.qos import QoSProfile\r\nimport threading\r\nimport time\r\n\r\nclass ConversationalRobotNode(Node):\r\n    def __init__(self):\r\n        super().__init__('conversational_robot')\r\n        \r\n        # Initialize all system components\r\n        self.multimodal_manager = MultimodalInteractionManager()\r\n        self.social_engine = SocialRoboticsEngine()\r\n        self.dialogue_state_tracker = DialogueStateTracker()\r\n        \r\n        # ROS 2 interfaces\r\n        self.speech_subscriber = self.create_subscription(\r\n            String, '/speech_input', self.speech_callback, 10\r\n        )\r\n        self.camera_subscriber = self.create_subscription(\r\n            Image, '/camera/rgb/image_raw', self.camera_callback, 10\r\n        )\r\n        self.speech_publisher = self.create_publisher(\r\n            String, '/speech_output', 10\r\n        )\r\n        self.motion_publisher = self.create_publisher(\r\n            Twist, '/cmd_vel', 10\r\n        )\r\n        \r\n        # System state\r\n        self.conversation_active = False\r\n        self.current_user = None\r\n        self.interaction_mode = 'attentive'  # attentive, reactive, idle\r\n        \r\n        # Start background processes\r\n        self.multimodal_manager.start_listening()\r\n        self.multimodal_manager.start_vision_processing()\r\n        self.multimodal_manager.start_response_processing()\r\n        \r\n        # Timer for periodic tasks\r\n        self.system_timer = self.create_timer(1.0, self.system_monitor)\r\n        \r\n        self.get_logger().info('Conversational Robot Node initialized')\r\n    \r\n    def speech_callback(self, msg):\r\n        \"\"\"\r\n        Handle incoming speech messages\r\n        \"\"\"\r\n        user_input = msg.data\r\n        self.get_logger().info(f'Processing speech: {user_input}')\r\n        \r\n        # Process the speech input through our system\r\n        response = self.process_conversation_turn(user_input)\r\n        \r\n        if response:\r\n            # Publish the response\r\n            response_msg = String()\r\n            response_msg.data = response\r\n            self.speech_publisher.publish(response_msg)\r\n            \r\n            # Also speak it out loud (in a real system, this would go to TTS)\r\n            self.get_logger().info(f'Robot says: {response}')\r\n    \r\n    def camera_callback(self, msg):\r\n        \"\"\"\r\n        Handle incoming camera data\r\n        \"\"\"\r\n        # Process camera data for visual context\r\n        # In a real system, this would convert ROS Image to OpenCV and process\r\n        self.get_logger().debug('Received camera data')\r\n        \r\n        # Update visual context in multimodal manager\r\n        # This would include object detection, face recognition, etc.\r\n        pass\r\n    \r\n    def process_conversation_turn(self, user_input: str):\r\n        \"\"\"\r\n        Process a complete conversation turn\r\n        \"\"\"\r\n        # Add to dialogue history\r\n        self.dialogue_state_tracker.conversation_history.append({\r\n            'timestamp': time.time(),\r\n            'speaker': 'user',\r\n            'text': user_input,\r\n            'intent': None,\r\n            'entities': {}\r\n        })\r\n        \r\n        # Process through dialogue system\r\n        dialogue_result = self.multimodal_manager.dialogue_manager.process_input(user_input)\r\n        \r\n        intent = dialogue_result.get('intent', 'unknown')\r\n        entities = dialogue_result.get('entities', {})\r\n        \r\n        # Update dialogue state\r\n        self.dialogue_state_tracker.update_context(\r\n            user_input=user_input,\r\n            robot_response=\"\",\r\n            intent=intent,\r\n            entities=entities\r\n        )\r\n        \r\n        # Generate response based on intent and context\r\n        response = self.generate_response(intent, entities, user_input)\r\n        \r\n        # Update conversation history with our response\r\n        self.dialogue_state_tracker.conversation_history.append({\r\n            'timestamp': time.time(),\r\n            'speaker': 'robot',\r\n            'text': response,\r\n            'intent': intent,\r\n            'entities': entities\r\n        })\r\n        \r\n        # Execute any required actions\r\n        self.execute_actions(intent, entities)\r\n        \r\n        return response\r\n    \r\n    def generate_response(self, intent: str, entities: Dict, user_input: str):\r\n        \"\"\"\r\n        Generate response based on intent and context\r\n        \"\"\"\r\n        if intent == 'greeting':\r\n            # Use social engine to generate appropriate greeting\r\n            context = {'time_of_day': 'day'}  # Would come from system clock\r\n            return self.social_engine.handle_greeting(\r\n                user_id=self.current_user or 'unknown', \r\n                context=context\r\n            )\r\n        elif intent == 'navigation':\r\n            location = entities.get('location', 'destination')\r\n            if location:\r\n                return f\"Okay, I'll navigate to the {location}. Please follow me.\"\r\n            else:\r\n                return \"I can help you navigate, but I need to know where you'd like to go.\"\r\n        elif intent == 'object_interaction':\r\n            obj = entities.get('object', 'object')\r\n            if obj:\r\n                return f\"I can help you with the {obj}. Can you show me where it is?\"\r\n            else:\r\n                return \"I can help you interact with objects. What would you like me to help you with?\"\r\n        else:\r\n            return \"I'm here to help. How can I assist you today?\"\r\n    \r\n    def execute_actions(self, intent: str, entities: Dict):\r\n        \"\"\"\r\n        Execute physical actions based on intent\r\n        \"\"\"\r\n        if intent == 'navigation':\r\n            location = entities.get('location')\r\n            if location:\r\n                self.navigate_to_location(location)\r\n        elif intent == 'object_interaction':\r\n            obj = entities.get('object')\r\n            if obj:\r\n                self.approach_object(obj)\r\n    \r\n    def navigate_to_location(self, location: str):\r\n        \"\"\"\r\n        Execute navigation to a specific location\r\n        \"\"\"\r\n        # In a real implementation, this would:\r\n        # 1. Check if location is known\r\n        # 2. Plan a path to the location\r\n        # 3. Execute navigation commands\r\n        # 4. Monitor progress\r\n        self.get_logger().info(f'Navigating to {location}')\r\n        \r\n        # For simulation, send a simple movement command\r\n        twist = Twist()\r\n        twist.linear.x = 0.2  # Move forward at 0.2 m/s\r\n        # In real system, this would be more sophisticated path following\r\n        for _ in range(10):  # Move for 10 seconds as simulation\r\n            self.motion_publisher.publish(twist)\r\n            time.sleep(1.0)\r\n        \r\n        # Stop the robot\r\n        stop_twist = Twist()\r\n        self.motion_publisher.publish(stop_twist)\r\n    \r\n    def approach_object(self, obj_name: str):\r\n        \"\"\"\r\n        Approach and interact with an object\r\n        \"\"\"\r\n        self.get_logger().info(f'Approaching {obj_name}')\r\n        # Implementation would involve:\r\n        # - Locating the object in space\r\n        # - Planning approach trajectory\r\n        # - Executing approach maneuver\r\n        pass\r\n    \r\n    def system_monitor(self):\r\n        \"\"\"\r\n        Monitor system state and perform periodic tasks\r\n        \"\"\"\r\n        # Check if conversation is still active\r\n        # Reset if no recent activity\r\n        if len(self.dialogue_state_tracker.conversation_history) > 0:\r\n            last_entry = self.dialogue_state_tracker.conversation_history[-1]\r\n            time_since_last = time.time() - last_entry['timestamp']\r\n            \r\n            # If no activity in 30 seconds, consider conversation ended\r\n            if time_since_last > 30 and self.conversation_active:\r\n                self.conversation_active = False\r\n                self.current_user = None\r\n                self.interaction_mode = 'idle'\r\n                self.get_logger().info('Conversation ended due to inactivity')\r\n    \r\n    def shutdown(self):\r\n        \"\"\"\r\n        Clean shutdown of the system\r\n        \"\"\"\r\n        self.get_logger().info('Shutting down conversational robot')\r\n        # Stop all background processes\r\n        # Save conversation history\r\n        # Clean up resources\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    \r\n    try:\r\n        robot = ConversationalRobotNode()\r\n        \r\n        # Run the node\r\n        rclpy.spin(robot)\r\n        \r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        robot.shutdown()\r\n        robot.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"76-summary",children:"7.6 Summary"}),"\n",(0,t.jsx)(n.p,{children:"This chapter has explored the complex field of conversational robotics, which integrates natural language processing, speech recognition, dialogue management, and multimodal interaction to create robots that can engage in human-like conversations. Key takeaways include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Conversational AI in robotics combines ASR, NLU, dialogue management, NLG, and TTS for natural interaction"}),"\n",(0,t.jsx)(n.li,{children:"Effective dialogue management requires state tracking, intent recognition, and context awareness"}),"\n",(0,t.jsx)(n.li,{children:"Multimodal interaction integrates speech, vision, and action for more natural communication"}),"\n",(0,t.jsx)(n.li,{children:"Social robotics principles help create engaging and appropriate robot behavior"}),"\n",(0,t.jsx)(n.li,{children:"Real conversational systems require integration of multiple complex components"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Conversational robotics represents a critical component of Physical AI systems, enabling more natural and intuitive human-robot interaction."}),"\n",(0,t.jsx)(n.h2,{id:"77-exercises",children:"7.7 Exercises"}),"\n",(0,t.jsx)(n.h3,{id:"exercise-71-speech-recognition-system",children:"Exercise 7.1: Speech Recognition System"}),"\n",(0,t.jsx)(n.p,{children:"Implement a robust speech recognition system that handles noise cancellation and keyword spotting for robot activation."}),"\n",(0,t.jsx)(n.h3,{id:"exercise-72-dialogue-state-tracker",children:"Exercise 7.2: Dialogue State Tracker"}),"\n",(0,t.jsx)(n.p,{children:"Create a dialogue state tracker that maintains conversation context across multiple turns and sessions."}),"\n",(0,t.jsx)(n.h3,{id:"exercise-73-intent-recognition",children:"Exercise 7.3: Intent Recognition"}),"\n",(0,t.jsx)(n.p,{children:"Build an intent recognition system that accurately identifies user intents and extracts relevant entities."}),"\n",(0,t.jsx)(n.h3,{id:"exercise-74-multimodal-integration",children:"Exercise 7.4: Multimodal Integration"}),"\n",(0,t.jsx)(n.p,{children:"Develop a system that combines speech and visual input to improve understanding and response generation."}),"\n",(0,t.jsx)(n.h3,{id:"exercise-75-complete-conversational-agent",children:"Exercise 7.5: Complete Conversational Agent"}),"\n",(0,t.jsx)(n.p,{children:"Build a complete conversational robot system that handles listening, processing, and responding appropriately."})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var t=r(6540);const i={},o=t.createContext(i);function s(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);