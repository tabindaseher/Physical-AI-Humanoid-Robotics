"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[9121],{191:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>l});var a=r(4848),i=r(8453);const s={sidebar_position:3,title:"Appendix C: Isaac ROS Tutorials"},t="Appendix C: Isaac ROS Tutorials",o={id:"appendices/appendix-c-isaac-ros-tutorials",title:"Appendix C: Isaac ROS Tutorials",description:"Overview of Isaac ROS",source:"@site/docs/appendices/appendix-c-isaac-ros-tutorials.md",sourceDirName:"appendices",slug:"/appendices/appendix-c-isaac-ros-tutorials",permalink:"/docs/appendices/appendix-c-isaac-ros-tutorials",draft:!1,unlisted:!1,editUrl:"https://github.com/tabindaseher/Physical-AI-Humanoid-Robotics/edit/main/docs/appendices/appendix-c-isaac-ros-tutorials.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,title:"Appendix C: Isaac ROS Tutorials"},sidebar:"tutorialSidebar",previous:{title:"Appendix B: Gazebo Setup",permalink:"/docs/appendices/appendix-b-gazebo-setup"},next:{title:"Appendix D: VLA Implementation",permalink:"/docs/appendices/appendix-d-vla-implementation"}},c={},l=[{value:"Overview of Isaac ROS",id:"overview-of-isaac-ros",level:2},{value:"Installation",id:"installation",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Setup",id:"setup",level:3},{value:"Getting Started with Isaac ROS Packages",id:"getting-started-with-isaac-ros-packages",level:2},{value:"Isaac ROS Apriltag",id:"isaac-ros-apriltag",level:3},{value:"Isaac ROS Stereo Image Processing",id:"isaac-ros-stereo-image-processing",level:3},{value:"Isaac ROS Visual Simultaneous Localization and Mapping (VSLAM)",id:"isaac-ros-visual-simultaneous-localization-and-mapping-vslam",level:3},{value:"Isaac ROS Navigation",id:"isaac-ros-navigation",level:2},{value:"Basic Navigation Node",id:"basic-navigation-node",level:3},{value:"Isaac ROS Object Detection",id:"isaac-ros-object-detection",level:2},{value:"YOLO-based Object Detection with Isaac ROS",id:"yolo-based-object-detection-with-isaac-ros",level:3},{value:"Isaac ROS with Isaac Sim",id:"isaac-ros-with-isaac-sim",level:2},{value:"Connecting Isaac ROS to Isaac Sim",id:"connecting-isaac-ros-to-isaac-sim",level:3},{value:"Launch Files for Isaac ROS",id:"launch-files-for-isaac-ros",level:2},{value:"Isaac ROS Stereo Example Launch",id:"isaac-ros-stereo-example-launch",level:3},{value:"Isaac ROS AprilTag Example Launch",id:"isaac-ros-apriltag-example-launch",level:3},{value:"TensorRT Integration with Isaac ROS",id:"tensorrt-integration-with-isaac-ros",level:2},{value:"Using TensorRT for Optimized Inference",id:"using-tensorrt-for-optimized-inference",level:3},{value:"Best Practices with Isaac ROS",id:"best-practices-with-isaac-ros",level:2},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Resource Management",id:"resource-management",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"appendix-c-isaac-ros-tutorials",children:"Appendix C: Isaac ROS Tutorials"}),"\n",(0,a.jsx)(n.h2,{id:"overview-of-isaac-ros",children:"Overview of Isaac ROS"}),"\n",(0,a.jsx)(n.p,{children:"Isaac ROS is NVIDIA's collection of hardware-accelerated packages that bring the power of NVIDIA's computing platforms to the Robot Operating System (ROS 2). These packages provide GPU-accelerated perception, navigation, and manipulation capabilities for robotics applications."}),"\n",(0,a.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,a.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"NVIDIA GPU with CUDA support (Compute Capability 6.0+)"}),"\n",(0,a.jsx)(n.li,{children:"Ubuntu 20.04 or 22.04"}),"\n",(0,a.jsx)(n.li,{children:"ROS 2 Humble Hawksbill"}),"\n",(0,a.jsx)(n.li,{children:"NVIDIA Container Toolkit"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"setup",children:"Setup"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Install Isaac ROS Developer Tools\r\nsudo apt update\r\nsudo apt install nvidia-isaa-ros-dev-tools\r\n\r\n# Or install specific packages individually\r\nsudo apt install nvidia-isaac-ros-perceptor\r\nsudo apt install nvidia-isaac-ros-isaac-ros-nav2\n"})}),"\n",(0,a.jsx)(n.h2,{id:"getting-started-with-isaac-ros-packages",children:"Getting Started with Isaac ROS Packages"}),"\n",(0,a.jsx)(n.h3,{id:"isaac-ros-apriltag",children:"Isaac ROS Apriltag"}),"\n",(0,a.jsx)(n.p,{children:"Apriltag detection accelerated on GPU for precise localization."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-cpp",children:'#include <rclcpp/rclcpp.hpp>\r\n#include <isaac_ros_apriltag_interfaces/msg/april_tag_detection_array.hpp>\r\n\r\nclass ApriltagNode : public rclcpp::Node\r\n{\r\npublic:\r\n    ApriltagNode() : Node("apriltag_node")\r\n    {\r\n        subscription_ = this->create_subscription<isaac_ros_apriltag_interfaces::msg::AprilTagDetectionArray>(\r\n            "tag_detections", 10,\r\n            std::bind(&ApriltagNode::detection_callback, this, std::placeholders::_1));\r\n    }\r\n\r\nprivate:\r\n    void detection_callback(const isaac_ros_apriltag_interfaces::msg::AprilTagDetectionArray::SharedPtr msg)\r\n    {\r\n        RCLCPP_INFO(this->get_logger(), "Detected %zu tags", msg->detections.size());\r\n        \r\n        for (const auto& detection : msg->detections) {\r\n            RCLCPP_INFO(this->get_logger(), \r\n                "Tag ID: %d, Position: (%.2f, %.2f, %.2f)",\r\n                detection.id, \r\n                detection.pose.pose.position.x,\r\n                detection.pose.pose.position.y,\r\n                detection.pose.pose.position.z);\r\n        }\r\n    }\r\n    \r\n    rclcpp::Subscription<isaac_ros_apriltag_interfaces::msg::AprilTagDetectionArray>::SharedPtr subscription_;\r\n};\n'})}),"\n",(0,a.jsx)(n.h3,{id:"isaac-ros-stereo-image-processing",children:"Isaac ROS Stereo Image Processing"}),"\n",(0,a.jsx)(n.p,{children:"Real-time stereo processing accelerated on GPU."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom stereo_msgs.msg import DisparityImage\r\n\r\nclass StereoProcessorNode(Node):\r\n    def __init__(self):\r\n        super().__init__('stereo_processor')\r\n        \r\n        # Subscribe to stereo image topics\r\n        self.left_sub = self.create_subscription(\r\n            Image, '/camera/left/image_rect', self.left_callback, 10)\r\n        self.right_sub = self.create_subscription(\r\n            Image, '/camera/right/image_rect', self.right_callback, 10)\r\n            \r\n        # Publish disparity map\r\n        self.disp_pub = self.create_publisher(\r\n            DisparityImage, '/disparity_map', 10)\r\n    \r\n    def left_callback(self, msg):\r\n        # Process left image\r\n        self.get_logger().info(f\"Received left image: {msg.width}x{msg.height}\")\r\n    \r\n    def right_callback(self, msg):\r\n        # Process right image\r\n        self.get_logger().info(f\"Received right image: {msg.width}x{msg.height}\")\n"})}),"\n",(0,a.jsx)(n.h3,{id:"isaac-ros-visual-simultaneous-localization-and-mapping-vslam",children:"Isaac ROS Visual Simultaneous Localization and Mapping (VSLAM)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-cpp",children:'#include <rclcpp/rclcpp.hpp>\r\n#include <nav_msgs/msg/odometry.hpp>\r\n#include <geometry_msgs/msg/pose_stamped.hpp>\r\n\r\nclass VSLAMNode : public rclcpp::Node\r\n{\r\npublic:\r\n    VSLAMNode() : Node("vslam_node")\r\n    {\r\n        // Subscribe to camera feed\r\n        image_subscription_ = this->create_subscription<sensor_msgs::msg::Image>(\r\n            "camera/image", 10,\r\n            std::bind(&VSLAMNode::image_callback, this, std::placeholders::_1));\r\n            \r\n        // Publish odometry\r\n        odom_publisher_ = this->create_publisher<nav_msgs::msg::Odometry>("odom", 10);\r\n        \r\n        // Publish pose\r\n        pose_publisher_ = this->create_publisher<geometry_msgs::msg::PoseStamped>("pose", 10);\r\n    }\r\n\r\nprivate:\r\n    void image_callback(const sensor_msgs::msg::Image::SharedPtr msg)\r\n    {\r\n        // Process visual SLAM with Isaac ROS VSLAM\r\n        // This would interface with Isaac ROS VSLAM package\r\n        RCLCPP_INFO(this->get_logger(), \r\n            "Processing frame for VSLAM: %dx%d", \r\n            msg->width, msg->height);\r\n            \r\n        // Publish odometry result\r\n        auto odom_msg = nav_msgs::msg::Odometry();\r\n        odom_msg.header.stamp = this->get_clock()->now();\r\n        odom_msg.header.frame_id = "map";\r\n        odom_publisher_->publish(odom_msg);\r\n    }\r\n    \r\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_subscription_;\r\n    rclcpp::Publisher<nav_msgs::msg::Odometry>::SharedPtr odom_publisher_;\r\n    rclcpp::Publisher<geometry_msgs::msg::PoseStamped>::SharedPtr pose_publisher_;\r\n};\n'})}),"\n",(0,a.jsx)(n.h2,{id:"isaac-ros-navigation",children:"Isaac ROS Navigation"}),"\n",(0,a.jsx)(n.p,{children:"GPU-accelerated navigation stack for mobile robots."}),"\n",(0,a.jsx)(n.h3,{id:"basic-navigation-node",children:"Basic Navigation Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom nav2_msgs.action import NavigateToPose\r\nfrom rclpy.action import ActionClient\r\n\r\nclass IsaacNavigationNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'isaac_navigation_node\')\r\n        \r\n        # Create action client for navigation\r\n        self.nav_client = ActionClient(\r\n            self, NavigateToPose, \'navigate_to_pose\')\r\n        \r\n        # Create publisher for goal poses\r\n        self.goal_publisher = self.create_publisher(\r\n            PoseStamped, \'goal_pose\', 10)\r\n    \r\n    def send_goal(self, x, y, z, w=1.0):\r\n        """Send navigation goal to Isaac ROS Navigation"""\r\n        goal_msg = NavigateToPose.Goal()\r\n        goal_msg.pose.header.frame_id = \'map\'\r\n        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\r\n        goal_msg.pose.pose.position.x = x\r\n        goal_msg.pose.pose.position.y = y\r\n        goal_msg.pose.pose.position.z = 0.0\r\n        goal_msg.pose.pose.orientation.z = z\r\n        goal_msg.pose.pose.orientation.w = w\r\n        \r\n        # Wait for action server\r\n        self.nav_client.wait_for_server()\r\n        \r\n        # Send goal\r\n        send_goal_future = self.nav_client.send_goal_async(goal_msg)\r\n        send_goal_future.add_done_callback(self.goal_response_callback)\r\n    \r\n    def goal_response_callback(self, future):\r\n        """Handle goal response"""\r\n        goal_handle = future.result()\r\n        if not goal_handle.accepted:\r\n            self.get_logger().info(\'Goal rejected\')\r\n            return\r\n            \r\n        self.get_logger().info(\'Goal accepted\')\r\n        get_result_future = goal_handle.get_result_async()\r\n        get_result_future.add_done_callback(self.get_result_callback)\r\n    \r\n    def get_result_callback(self, future):\r\n        """Handle navigation result"""\r\n        result = future.result().result\r\n        self.get_logger().info(f\'Navigation completed: {result}\')\n'})}),"\n",(0,a.jsx)(n.h2,{id:"isaac-ros-object-detection",children:"Isaac ROS Object Detection"}),"\n",(0,a.jsx)(n.h3,{id:"yolo-based-object-detection-with-isaac-ros",children:"YOLO-based Object Detection with Isaac ROS"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom std_msgs.msg import Header\r\n\r\nclass IsaacObjectDetectionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('isaac_object_detection_node')\r\n        \r\n        # Subscribe to camera feed\r\n        self.image_subscription = self.create_subscription(\r\n            Image, 'camera/image_raw', self.image_callback, 10)\r\n        \r\n        # Publish detections\r\n        self.detection_publisher = self.create_publisher(\r\n            Detection2DArray, 'detections', 10)\r\n    \r\n    def image_callback(self, msg: Image):\r\n        \"\"\"\r\n        Process image and detect objects using Isaac ROS\r\n        This is a simplified example - real implementation would interface\r\n        with Isaac ROS object detection packages\r\n        \"\"\"\r\n        # Simulate object detection results\r\n        detection_array = Detection2DArray()\r\n        detection_array.header = Header()\r\n        detection_array.header.stamp = self.get_clock().now().to_msg()\r\n        detection_array.header.frame_id = msg.header.frame_id\r\n        \r\n        # Example detection (in reality, this would come from Isaac ROS AI model)\r\n        if True:  # Condition would be based on actual detection\r\n            detection = Detection2D()\r\n            detection.header = detection_array.header\r\n            \r\n            # Bounding box (normalized coordinates)\r\n            bbox = BoundingBox2D()\r\n            bbox.center.x = 0.5\r\n            bbox.center.y = 0.5\r\n            bbox.size_x = 0.2\r\n            bbox.size_y = 0.3\r\n            detection.bbox = bbox\r\n            \r\n            # Class and confidence\r\n            hypothesis = ObjectHypothesisWithPose()\r\n            hypothesis.id = \"person\"\r\n            hypothesis.score = 0.95\r\n            detection.results.append(hypothesis)\r\n            \r\n            detection_array.detections.append(detection)\r\n        \r\n        # Publish detections\r\n        self.detection_publisher.publish(detection_array)\r\n        self.get_logger().info(f'Published {len(detection_array.detections)} detections')\n"})}),"\n",(0,a.jsx)(n.h2,{id:"isaac-ros-with-isaac-sim",children:"Isaac ROS with Isaac Sim"}),"\n",(0,a.jsx)(n.h3,{id:"connecting-isaac-ros-to-isaac-sim",children:"Connecting Isaac ROS to Isaac Sim"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2\r\nfrom geometry_msgs.msg import Twist\r\nimport numpy as np\r\n\r\nclass IsaacSimBridgeNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'isaac_sim_bridge\')\r\n        \r\n        # Publishers for Isaac Sim\r\n        self.cmd_vel_publisher = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        \r\n        # Subscribers from Isaac Sim\r\n        self.rgb_subscription = self.create_subscription(\r\n            Image, \'/camera/rgb/image_raw\', self.rgb_callback, 10)\r\n            \r\n        self.depth_subscription = self.create_subscription(\r\n            Image, \'/camera/depth/image_raw\', self.depth_callback, 10)\r\n            \r\n        self.camera_info_subscription = self.create_subscription(\r\n            CameraInfo, \'/camera/rgb/camera_info\', self.camera_info_callback, 10)\r\n    \r\n    def rgb_callback(self, msg: Image):\r\n        """Process RGB camera data from Isaac Sim"""\r\n        # Convert ROS Image to format usable by Isaac ROS packages\r\n        # Process with Isaac ROS perception algorithms\r\n        self.get_logger().info(f\'Received RGB image: {msg.width}x{msg.height}\')\r\n    \r\n    def depth_callback(self, msg: Image):\r\n        """Process depth camera data from Isaac Sim"""\r\n        # Process depth data with Isaac ROS algorithms\r\n        self.get_logger().info(f\'Received depth image: {msg.width}x{msg.height}\')\r\n    \r\n    def camera_info_callback(self, msg: CameraInfo):\r\n        """Process camera calibration data"""\r\n        # Use calibration data for Isaac ROS stereo or monocular processing\r\n        self.get_logger().info(f\'Camera calibration received\')\r\n    \r\n    def send_velocity_command(self, linear_x: float, angular_z: float):\r\n        """Send velocity command to robot in Isaac Sim"""\r\n        cmd_msg = Twist()\r\n        cmd_msg.linear.x = linear_x\r\n        cmd_msg.angular.z = angular_z\r\n        self.cmd_vel_publisher.publish(cmd_msg)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"launch-files-for-isaac-ros",children:"Launch Files for Isaac ROS"}),"\n",(0,a.jsx)(n.h3,{id:"isaac-ros-stereo-example-launch",children:"Isaac ROS Stereo Example Launch"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\r\nfrom launch_ros.actions import ComposableNodeContainer\r\nfrom launch_ros.descriptions import ComposableNode\r\n\r\ndef generate_launch_description():\r\n    # Isaac ROS Stereo Disparity container\r\n    stereo_disparity_container = ComposableNodeContainer(\r\n        name='stereo_disparity_container',\r\n        namespace='',\r\n        package='rclcpp_components',\r\n        executable='component_container_mt',\r\n        composable_node_descriptions=[\r\n            ComposableNode(\r\n                package='isaac_ros_stereo_image_proc',\r\n                plugin='isaac_ros::stereo_image_proc::DisparityNode',\r\n                name='disparity_node',\r\n                parameters=[{\r\n                    'approximate_sync': True,\r\n                    'use_system_default_qos': True\r\n                }],\r\n                remappings=[\r\n                    ('left/image_rect', '/camera/left/image_rect'),\r\n                    ('right/image_rect', '/camera/right/image_rect'),\r\n                    ('left/camera_info', '/camera/left/camera_info'),\r\n                    ('right/camera_info', '/camera/right/camera_info'),\r\n                ]\r\n            )\r\n        ]\r\n    )\r\n\r\n    return LaunchDescription([stereo_disparity_container])\n"})}),"\n",(0,a.jsx)(n.h3,{id:"isaac-ros-apriltag-example-launch",children:"Isaac ROS AprilTag Example Launch"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\r\nfrom launch.actions import DeclareLaunchArgument\r\nfrom launch.substitutions import LaunchConfiguration\r\nfrom launch_ros.actions import ComposableNodeContainer\r\nfrom launch_ros.descriptions import ComposableNode\r\n\r\ndef generate_launch_description():\r\n    launch_args = [\r\n        DeclareLaunchArgument(\r\n            'image_width',\r\n            default_value='640',\r\n            description='Width of input images'),\r\n        DeclareLaunchArgument(\r\n            'image_height', \r\n            default_value='480',\r\n            description='Height of input images'),\r\n    ]\r\n\r\n    image_width = LaunchConfiguration('image_width')\r\n    image_height = LaunchConfiguration('image_height')\r\n\r\n    apriltag_container = ComposableNodeContainer(\r\n        name='apriltag_container',\r\n        namespace='apriltag',\r\n        package='rclcpp_components',\r\n        executable='component_container_mt',\r\n        composable_node_descriptions=[\r\n            ComposableNode(\r\n                package='isaac_ros_apriltag',\r\n                plugin='nvidia::isaac_ros::apriltag::AprilTagNode',\r\n                name='apriltag_node',\r\n                parameters=[{\r\n                    'size': 0.32,  # Tag size in meters\r\n                    'max_tags': 16,\r\n                    'family': 'tag36h11',\r\n                }],\r\n                remappings=[\r\n                    ('image', '/image_rect'),\r\n                    ('camera_info', '/camera_info'),\r\n                ]\r\n            )\r\n        ],\r\n        output='screen'\r\n    )\r\n\r\n    return LaunchDescription(\r\n        launch_args + [apriltag_container]\r\n    )\n"})}),"\n",(0,a.jsx)(n.h2,{id:"tensorrt-integration-with-isaac-ros",children:"TensorRT Integration with Isaac ROS"}),"\n",(0,a.jsx)(n.h3,{id:"using-tensorrt-for-optimized-inference",children:"Using TensorRT for Optimized Inference"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nimport torch\r\nimport tensorrt as trt\r\nimport pycuda.driver as cuda\r\nimport numpy as np\r\n\r\nclass TensorRTInferenceNode(Node):\r\n    def __init__(self):\r\n        super().__init__('tensorrt_inference_node')\r\n        \r\n        # Initialize TensorRT engine\r\n        self.engine = self.load_tensorrt_engine('/path/to/model.plan')\r\n        self.context = self.engine.create_execution_context()\r\n        \r\n        # Allocate buffers\r\n        self.inputs, self.outputs, self.bindings, self.stream = self.allocate_buffers()\r\n        \r\n    def load_tensorrt_engine(self, engine_path):\r\n        \"\"\"Load a pre-built TensorRT engine\"\"\"\r\n        with open(engine_path, 'rb') as f:\r\n            engine_data = f.read()\r\n        \r\n        runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))\r\n        engine = runtime.deserialize_cuda_engine(engine_data)\r\n        return engine\r\n    \r\n    def allocate_buffers(self):\r\n        \"\"\"Allocate buffers for TensorRT engine\"\"\"\r\n        inputs = []\r\n        outputs = []\r\n        bindings = []\r\n        stream = cuda.Stream()\r\n        \r\n        for idx in range(self.engine.num_bindings):\r\n            print(f\"Binding {idx}: {self.engine.get_binding_name(idx)}\")\r\n            print(f\"Binding {idx} shape: {self.engine.get_binding_shape(idx)}\")\r\n            \r\n            binding_shape = self.engine.get_binding_shape(idx)\r\n            size = trt.volume(binding_shape) * self.engine.max_batch_size * np.dtype(np.float32).itemsize\r\n            \r\n            # Allocate host and device buffers\r\n            host_mem = cuda.pagelocked_empty(size, dtype=np.float32)\r\n            device_mem = cuda.mem_alloc(host_mem.nbytes)\r\n            \r\n            bindings.append(int(device_mem))\r\n            \r\n            if self.engine.binding_is_input(idx):\r\n                inputs.append({'host': host_mem, 'device': device_mem})\r\n            else:\r\n                outputs.append({'host': host_mem, 'device': device_mem})\r\n        \r\n        return inputs, outputs, bindings, stream\r\n    \r\n    def do_inference(self, input_data):\r\n        \"\"\"Perform TensorRT inference\"\"\"\r\n        # Copy input data to GPU\r\n        np.copyto(self.inputs[0]['host'], input_data.ravel())\r\n        cuda.memcpy_htod_async(self.inputs[0]['device'], self.inputs[0]['host'], self.stream)\r\n        \r\n        # Execute inference\r\n        self.context.execute_async_v2(bindings=self.bindings, stream_handle=self.stream.handle)\r\n        \r\n        # Copy output data back to CPU\r\n        cuda.memcpy_dtoh_async(self.outputs[0]['host'], self.outputs[0]['device'], self.stream)\r\n        self.stream.synchronize()\r\n        \r\n        return self.outputs[0]['host'].copy()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices-with-isaac-ros",children:"Best Practices with Isaac ROS"}),"\n",(0,a.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Use appropriate QoS settings"})," for real-time performance"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Minimize data copying"})," between host and device memory"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Batch operations"})," where possible to maximize GPU utilization"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Profile your application"})," to identify bottlenecks"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Implement safety checks"})," before executing actions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Validate sensor data"})," before using it for navigation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Monitor GPU utilization"})," and temperature"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Implement fallback strategies"})," when acceleration is not available"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"resource-management",children:"Resource Management"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Manage GPU memory"})," carefully to avoid out-of-memory errors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Use appropriate precision"})," (FP16 vs FP32) based on application needs"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Monitor power consumption"})," for mobile robots"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Implement proper cleanup"})," of GPU resources"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>o});var a=r(6540);const i={},s=a.createContext(i);function t(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);